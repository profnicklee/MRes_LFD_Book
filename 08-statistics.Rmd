# Introduction to Statistics {#statistics}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)

library(TeachingDemos)
```

This Chapter builds on the material in Lesson 9.

Here, we begin to apply our probability concepts to the task of statistical inference in the 'classical' way.

The theory explained and demonstrated here is the foundation of the (very large) majority of the quantitative business and management research you will read and use. Almost any time someone refers to a finding being **(statistically) significant** or a **p-value**, they are using this idea. However, a major weakness in many social science fields - business and management being one - is that a rather large proportion of researchers don't actually understand the core principles of statistical inference, and are instead often just copying what other people have written, or told them to do.

So, if you can get your head around the next 3-4 chapters, you'll be in a strong position to work with what's already out there, and also do your own solid research.

## The Distribution of Sample Means, and the Central Limit Theorem

The first thing to cover is an extension of the ideas in the last chapter about sampling distributions. It's called the *Central Limit Theorem*, and is often the source of much angst in statistics students (including myself). However, here I am actually going to demonstrate how it works, and show how is actually quite amazing.

To do so, I will use a synthetic data set of monthly incomes from 20 people. Let us treat this data set as the **population**.

```{r}
CLT<-read_excel("D:/Dropbox/R_Files/Data/CLT.xlsx")
head(CLT)
summary(CLT$Income)
sd(CLT$Income)
```

Above, you'll see the important descriptive statistics, including the standard deviation of 2366.432. Let's plot the frequency distribution, and once we do so we will find that it is essentially uniform - every value occurs once and only once in the population.

```{r}
ggplot(CLT, aes(x=Income))+geom_histogram(binwidth=400, colour="black", fill="white")
```

Now, what I am going to do is take a *single random sample of 2* from that population. This sample is taken 'without replacement', if you are wondering, and so it is in essence exactly as if a researcher went into the field and took a sample from this population.

```{r}

vec <- CLT$Income
  
# generating 1 random combination of 2 of the 
# Income values 
print ("One Random Combination of 2 of the 20 Income Values")
res1<- sample(vec,2,replace=FALSE)
print (res1)


```

Let's take the mean of the sample:

```{r}
data1<-data.frame(res1)

head(data1)

mean(res1)
```

The mean of the random sample of 2 will of course be different each time the sample is taken (e.g. when writing this text, the mean was 10600, but this may not be the same in the version you are reading!)

Just for reference, the mean of the population (remember, the 20 people) was 9400

So, let's now do that for **every possible combination of 2 values** from this population.

Given there are n=20 values in the population, there are k=190 possible combinations of two values.

Note: This is not the same as bootstrapping - we are *not sampling with replacement* here. We are instead taking combinations. The thing to think about is that this is the equivalent of taking every single possible sample of 2 that you could take from this population.

Remember the ping-pong balls!

Let's do it:

```{r}
##code modified from: https://www.geeksforgeeks.org/calculate-combinations-and-permutations-in-r/
vec <- CLT$Income
  
# generating combinations of the 
# Income values taking 2 at a time
print ("Every Possible Combination of the 20 Income Values")

res<- combinations(n= 20, r = 2, v = vec)
print (res)
  
print ("Number of combinations without repetition")

print (nrow(res))
```

That was a rather long exhibit up there, but I wanted to show you that this was actually really done properly, and we have 190 combinations.

Next, let us create the means of each of the 190 combinations

```{r}
data<-data.frame(res)
data$MDIST <- rowMeans(data)

head(data)
```

That's just the first 6 (the head of the data set), but you can rest assured that there are 190 means created here.

OK, so here is the kicker. Let's plot a histogram of these 190 means.

Remember, a histogram is a frequency distribution:

```{r}
ggplot(data, aes(x=MDIST))+geom_histogram(bins=13, colour="black", fill="white")

```

Well well well!

Even though the original population was a completely uniform distribution with a mean of 9400, the distribution of all of the sample means looks quite a lot like a normal distribution / bell curve!

Let's overlay one on it as well to make the point...

```{r}
ggplot(data, aes(x=MDIST))+geom_histogram(bins = 13, aes (y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")
```

Further, let's take the mean of those means...

```{r}
summary(data$MDIST)
```

Hello! It turns out, the mean of those means is the *population mean!!!*

This, in a demonstration, is *the central limit theorem.*

Let's **move back to the slides** to talk about this some more.

## Demonstrating Confidence Intervals {#conf}

Below I will demonstrate the idea of confidence intervals as a measure of precision, and at the same time help to really lock in the correct interpretation of what a confidence interval really means.

To do so, I will continue the in-class example of WBS Ph.D. graduate salaries. However, here, I will **simulate** the population distribution that we have sampled from. Let's recap:

Sample size (N) = 42

Sample mean = 170,000

Sample SD = 46617.4

I will simulate a population distribution that this sample could quite conceivably have come from. It will have the following features:

Population mean = 165,000

Population SD = 40000

I will also assume that the population distribution is *normal*. That is, the distribution of all business and management Ph.D. starting salaries is shaped like a bell curve - the normal distribution (remember birth weights from the last chapter). This is a reasonable assumption in my view, given the features of the phenomena we are investigating.

The important thing to remember is not really the population distribution of salaries (which is just used here to simulate the population), but the *sampling distribution of the means*. Remember, the central limit theorem shows us that **whatever the shape of the population distribution we are sampling from**, the sampling distribution of the means will take the form of a normal distribution, with a mean of the population mean.

So, below, I will simulate the population, and then calculate 100 confidence intervals for 4 different sample sizes (5, 10, 42, 100), to demonstrate two things:

a)  How confidence intervals get more precise as sample size increases, and
b)  How to interpret your specific confidence interval as simply one possibility of many.

```{R}

ci.examp <- function(mean.sim=100, sd=10, n=25, reps=50,
                     conf.level=0.95, method="z",
                     lower.conf=(1-conf.level)/2,
                     upper.conf=1-(1-conf.level)/2,
                     XLIM = 100 + 40*c(-1,1)) 
  {
data <- matrix( rnorm( n*reps, mean.sim, sd), ncol=n)
  rmeans <- rowMeans(data)
  switch(method, Z=,z={
    lower <- qnorm( lower.conf, rmeans, sd/sqrt(n))
    upper <- qnorm( upper.conf, rmeans, sd/sqrt(n))
  },
         T=,t= {
           cv.l <- qt(lower.conf, n-1)
           cv.u <- qt(upper.conf, n-1)
           rsds <- sqrt( apply(data,1,var) )/sqrt(n)

           lower <- rmeans+cv.l*rsds
           upper <- rmeans+cv.u*rsds
         },
         BOTH=, Both=, both={
           lz <- qnorm( lower.conf, rmeans, sd/sqrt(n))
           uz <- qnorm( upper.conf, rmeans, sd/sqrt(n))

           cv.l <- qt(lower.conf, n-1)
           cv.u <- qt(upper.conf, n-1)
           rsds <- sqrt( apply(data,1,var) )/sqrt(n)

           lt <- rmeans+cv.l*rsds
           ut <- rmeans+cv.u*rsds

           lower <- c(rbind(lt,lz,mean.sim))
           upper <- c(rbind(ut,uz,mean.sim))

           reps <- reps*3
           rmeans <- rep(rmeans, each=3)
           rmeans[c(F,F,T)] <- NA

         },
         stop("method must be z, t, or both") )

  if( any( upper==Inf ) ) upper <- rep( 2*mean.sim-min(lower), reps )
  if( any( lower==-Inf ) ) lower <- rep( 2*mean.sim-max(upper), reps )

  xr <- range( upper, lower )

  plot(lower,seq(1,reps), type="n",
       xlim=XLIM,       ## Changed
       xlab="Confidence Interval",
       ylab="Index",)

  ## abline( v= qnorm(c(1-upper.conf,1-lower.conf), mean.sim, sd/sqrt(n)), col=10) ## Deleted

  title(paste("Sample size is", n, "each"))     ## Changed

  colr <- ifelse( lower > mean.sim, 5, ifelse( upper < mean.sim, 6, 1) )

  abline(v=mean.sim)

  for( i in seq(1,reps) ){

    segments(lower[i], i, upper[i], i, col=colr[i])

  }

  points( rmeans, seq(along=rmeans), pch="|" )
  invisible(NULL)
}

```

```{R}
## Example numbers from slide deck - imaginary WBS PhD Salaries sample was 170K, SD was 46617.4
POPULATION.MEAN <- 165
POPULATION.SD <- 40

## Repeat at different sample size including 42 from example in class
junk <- lapply(c(5,10,42,100), function(N){
    ci.examp(mean.sim   = POPULATION.MEAN,
             sd         = POPULATION.SD,
             n          = N,
             reps       = 100,
             conf.level = 0.95,
             method     = "z",
             XLIM       = POPULATION.MEAN + 2 * POPULATION.SD * c(-1, 1)
             )
})

```

So, you can see that the confidence intervals for each of the 100 samples at a given sample size all cluster around the mean, but are wider as sample size is lower. That makes sense intuitively.

Looking specifically at our example N of 42, we can estimate which one of the 100 that might be, and see how the CI for sample means of about that size does include the real population mean of 165. You can also see there are a number of coloured CIs which do **not** include the sample mean. This shows how to interpret a CI.

Specifically, **your actual sample** is one of a very large amount of possible samples from the population (here we took 100 but we could have taken many more). The CI calculation essentially creates a CI with the width so that, out of 100 random samples, approximately 95 will actually contain the *true population mean*.

As such, we can see how it is clearly a measure of the **precision** of the estimate. For a compelling visualization of this, look how wide the CIs for N=5 are.

One thing to remember though is the 'law of large numbers' here. You might notice that not all of the CI plots contain exactly 5 intervals that do not contain the mean. This is analagous to the idea of coin flipping - it's just probability over very large numbers of trials, not saying that 'out of every 100, exactly 5 will not include the population mean'. But, over a very large number of trials, we can expect the amount of intervals that do not include the mean to tend towards 5%, in the same way that over a large number of coin flips, the proportion of heads tends towards 50%.
