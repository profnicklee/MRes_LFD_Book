# Classical Statistical Hypothesis Testing {#H-testing}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)
library(skimr)
library(rstatix)
library(pwr)
```

This Chapter relates to the first part of Lecture 8, and contains some (re)worked examples from earlier chapters / lessons, focusing specifically on the hypothesis testing aspect of them.

## Premier League Goals and the Poisson Distribution

In this section, we are not really worried about the *actual* goals or anything like that. What we are concerned about is formally testing the hypothesis that Premier League goals per game actually do follow a Poisson distribution. To do this, we will use what is called a **chi-square** test, which uses the chi-square statistic.

Chi-square is a fantastically versatile statistic, which is often used to compare tables of observations. Commonly, we compare a table of observed events with a table of events that we would expect if the null hypothesis were true in the population. The chi-square value is the measure of the difference between observed and expected counts in the table. We can use it here to compare the amount of goals we actually did observe in the 21-22 season, with those that we would *expect* to observe if the goals per game really do follow a Poisson distribution.

Again, much of the idea and code for this section comes from: <https://bookdown.org/theqdata/honors_thesis/goal-scoring-and-the-poisson-process.html>

The data comes from: <https://www.football-data.co.uk/englandm.php> and is the EPL results for 21-22 Season, with a new column of Total Goals (TOTG) created by me in Excel.

```{r}
EPL<-read_excel("Data/EPL21-22.xlsx")
head(EPL)
```

```{r}
summary(EPL$HomeTeam)
```

The above is a quick way of calculating / checking the total number of games if you don't know already.

```{r}
summary(EPL$TOTG)
```

```{r}
sum(EPL$TOTG)
```

First, let's create a table of all the matches with different numbers of goals, just as we did in Chapter 7.

```{r}
GoalsTable <- 
  EPL %>% 
  group_by(TOTG) %>% 
  summarise(ActualMatches = n())
GoalsTable 
```

Here, we have a problem. The chi-square test has a condition that the expected values in every column will be greater than 5, and as we go down the Poisson probabilities (i.e. for higher total goals), it is clear that this will start to become a problem for us.

The solution is simple, we combine the numbers for larger numbers of goals into a single 'x and above' variable. Here, let's do it for 6 goals and above.

To do so, we first need to create a new table, where the values for 6 and above are combined into a new row.

```{r}
# select first 5 rows (0, 1, 2, 3, 4, 5 goals)
NewGoalsTable <- GoalsTable[1:6,]
# sum up the remaining rows
NewGoalsTable[7,] <- sum(GoalsTable[7:nrow(GoalsTable),2])
NewGoalsTable <- mutate(NewGoalsTable, TOTG = as.character(TOTG))
# put in 1 category called "5 or more"
NewGoalsTable[7,"TOTG"] <- "6 or more" 
NewGoalsTable
```

Now we can create the Poisson distribution for the relevant mean, which means we first need to create the stats...

```{r}
fav_stats(EPL$TOTG)
```

Below is some code which will help us to create the distributions and also check them.

```{r}
MeanGoals <- fav_stats(EPL$TOTG)[[6]]
numMatches <- fav_stats(EPL$TOTG)[[8]]
StDevGoals <- fav_stats(EPL$TOTG)[[7]]
VarianceGoals <- StDevGoals ^ 2
```

```{r}
MeanGoals
```

```{r}
VarianceGoals
```

As we saw in Chapter 7, while for a Poisson distribution these should be exactly the same, we decided that they were good enough to be going on with.

So, let's actually check this formally with the Chi-square test.

We do so by first creating a table of the Poisson probabilities for x or fewer goals, for the Poisson with a mean of 2.818 (i.e. the variable MeanGoals)

```{r}
PoisProb <- dpois(c(0:6), MeanGoals)
POIS <-data.frame(PoisProb)
POIS
```

This allows us to make some predictions of what we could expect.

Remembering again that there are 380 games in a season, we can see that there is a 0.16 chance of seeing 4 goals in a game, which equates to 380 x 0.16 = 60.8 games we would expect to see in a season with 4 goals.

We can check this out by creating a new table comparing actual with predicted values...

```{r}
NewGoalsTable <- cbind(NewGoalsTable, PoisProb) 
NewGoalsTable <- mutate(NewGoalsTable, 
                        ExpectedMatches = round(numMatches * PoisProb))
NewGoalsTable
```

This looks quite frighteningly close up until you get to the 5 row. It will be interesting to see how we go with a formal chi-square test.

```{r}
TOTGChisq <- chisq.test(NewGoalsTable$ActualMatches, 
                      p = NewGoalsTable$PoisProb, rescale.p = TRUE)
TOTGChisq
```

The P-value here is actually 0.64. This suggests that there is not a significant difference between the distribution of the actual goals, and that which would be expected if they did follow a Poisson distribution.

## Correlation Significance Tests

Here, we revisit our correlation between GDP per capita, and Happiness metrics, which I pulled from Our Word in Data:

```{r}
Happy<-read_excel("Data/HappyGDP.xlsx", sheet = "2020")

summary(Happy)
head(Happy)
```

Let's not worry about plotting the data, and go straight to the correlation:

```{r}
Assoc1 <- cor.test(Happy$Happiness, Happy$GDPpc, 
                    method = "pearson")

Assoc1

```

Here are our results. The estimate is a correlation, and we test that using the **t statistic**. The *t-value* is simply the estimate divided by the standard error (which we can't see in this output), and is interpreted essentially as 'how far from 0 is the estimate, in standard errors'.

The *p*-value for t is very very small, and obviously less than 0.05.

Conclusion - reject null hypothesis, accept alternative hypothesis (as always, pending better evidence).

Importantly, this does not mean that the **true correlation in the population** is 0.745, simply that it is *very unlikely to be zero*.

We can then look at our estimate of 0.745, and - even better - our confidence interval (see Section \@ref(conf) for information on how to interpret confidence intervals), to gain some indication of the likely true correlation in the population.

## Regression Significance Tests

The process to asses the significance of regression estimates is very very similar to that for correlations. Let's revisit the heart disease data set we used earlier.

```{r}
Heart<-read_excel("Data/heart.data.xlsx")

summary(Heart)
head(Heart)
```

Let's go straight to the multiple regression model.

```{r}
heart.disease.lm<-lm(heart.disease ~ biking + smoking, data = Heart)

summary(heart.disease.lm)
```

We interpret these just as we did the correlation significance tests.

The t-value is large, and the *p*-value (two-tailed) is small.

Interestingly, here we are given 'stars' for the different levels of significance, so to some extent the software is doing some decision making for you. To be honest, I always caution against relying solely on looking for 'stars' (it's actually a bit of a running joke amongst the people who have known me a long time that I once told an entire class in the 1990s to 'just look for the stars'). That's because the actual significance or not decision is based on the critical value and one- or two-tailed decision. The software often makes an assumption of 0.05 critical value for *p*, two-tailed, and calculates the 'stars' based on that. Sometimes that can conflict with the decision you have made yourself about what should be significant or not. That can trip you up if you didn't know to change these values in the software package.

Further, it also sort of entrenches the idea that things can be 'more' or 'less' statistically significant. Take a look at the results above, you'll see 3 stars represents a significance of '0', and 2 stars represents '0.001', boring old '0.05' only gets a single star, and '0.1' gets a dot. I'm not a fan here because this encourages the analyst to post-hoc make decisions about 'marginally' significant, or 'very' significant. These concepts do not actually exist in real statistical theory. Instead, you decide your critical value ***a priori***, and you either pass or fail it. As Yoda may have said if he was a statistician, "Pass or do not pass. There is no marginally significant".

Am I perfect? No. Specifically, do any of my papers use the language of 'marginal' significance? Sure, I bet you could find them. I am never a fan though, and I can promise you I argued about it at the time!
