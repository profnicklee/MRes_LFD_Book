---
title: "WBS MRes: Learning From Data"
author: "Professor Nick Lee"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This book contains the examples and notes for the WBS MrRes Learning from Data Course. It is based on the Bookdown Demo written by Hui. The output format for this example is bookdown::gitbook."
---

# Introduction and Orientation

This is an online course book, designed for students on the Warwick Business School MRes Programme, who are taking the module 'Learning from Data (and) Science'. The module consists of a set of in-person lectures, linked with these examples and additional expositions.

You can see to the left a list of the contents of the book. However, please note that these 'chapter numbers' do not refer to the specific lectures. Not all of the lectures use quantitative examples.


This book is written in **Markdown**, using R and the **bookdown** package, and published online in HTML format. A full archive of all the R Code and data is available from Professor Lee on request, once the book is completed for the year.

This book is essentially a 'live' document, meaning that it is finalised for each year just before the module runs, but then modified for future runnings.

The current year is 2024, and the book is currently *in progress for 2024.*

Please enjoy the book, and learn from it. 

Feel free to email me with any questions or comments: nick.lee@wbs.ac.uk


<!--chapter:end:index.Rmd-->

# Describing the World With Data {#intro}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(readxl)
library(ggplot2)
library(tidyverse)
library(hrbrthemes)
library(viridis)
library(ggridges)
library(summarytools)
```

This Chapter is designed to work with Lecture 2.

Here, I will demonstrate a number of important concepts about data displays and descriptions.

First, let's find some data to play with. In the following examples I will use some data from the Johns Hopkins Coronavirus Resource Center as at April 26 2022, which contains data by country about cases, deaths, and mortality for COVID.

<https://coronavirus.jhu.edu/>

However, as of 2023, the JHCRC ceased collecting data, and their archival data is available on their github:

<https://github.com/CSSEGISandData/COVID-19>

However, I do think it would be quite hard to recreate the exact snapshot in time of the data set we are using here.

I'm going to use the top 20 countries in terms of raw number of cases

```{r}
COVID <- read_excel("Data/COVID.xlsx",sheet = "Top 20")
```

To demonstrate how we can easily change people's views of a data set, let's run some bar charts:

```{r}
p <- ggplot(COVID,aes(NAME,S_RATE))
p2 <- p +geom_bar(stat="identity")+theme(axis.text.x = element_text(angle = 90))+scale_x_discrete(name="Country")+scale_y_continuous(name = "Survival Rate")
p2
```

One thing we could do is change the orientation of the bars...

```{r}
p2 + coord_flip()
```

That's nice. The basic conclusion we can draw is everyone is pretty much the same in terms of survival rate, although Mexico is a bit down.

Interestingly, it seems to me that the variation is more pronounced in this presentation where you look 'down' a line of results, than above where you look across. I'm not aware of any theory behind why, or even if anyone has ever noted that before, but it's definitely something I noticed here.

Either way, if I wanted to make everyone think that COVID survival rates were really different, there's a simple trick I could use:

```{r}
# ggplot(COVID,aes(NAME,S_RATE))+geom_bar(stat="identity")+coord_flip(ylim=c(90,100))

p2 + coord_cartesian(ylim = c(90, 100))
```

Wow, how do you like that! What's different?

Changing the scale of the y-axis is an old trick, and when used to manipulate opinion it's a bad thing. However, there is a case to say we need to know what we want to say, before we decide how to say it. When does 'making sure we see the right message' move into 'outright manipulation / misrepresentation?'.

Here's another common way this type of data is presented in media and non-scientific reports:

```{r}
#ggplot(COVID,aes(NAME,S_RATE, group=1))+geom_line()+geom_point()
p + aes(group=1) + geom_line()+geom_point() +theme(axis.text.x = element_text(angle = 90))+scale_x_discrete(name="Country")+scale_y_continuous(name = "Survival Rate")
```

or a variant:

```{r}
#ggplot(COVID,aes(NAME,S_RATE, group=1))+geom_step()+geom_point()
p + aes(group=1) + geom_step()+geom_point() +theme(axis.text.x = element_text(angle = 90))+scale_x_discrete(name="Country")+scale_y_continuous(name = "Survival Rate")
```

Technically, it's wrong to present this data in a line chart, because it is discrete values and does not represent a trend, but it is surprisingly common - probably because it 'looks more sciency'...

## Summarizing Data with Numbers

Above, we simply plotted a single variable of interest, and showed that some very basic changes to how it was displayed had the potential to 'lead' the reader to different conclusions. Even that simple tweaking is a powerful way to change opinion, and used in the wrong way can be very misleading.

Now, let's think about the idea of an *even simpler* way to describe data - by summarizing it with just some numbers.

For this example, I am going to use data on GDP per head in 2020 for each nation, from the World Bank.

```{r}
GDP <- read_excel("Data/2020GDP.xls",sheet = "Data")
```

Before I do anything else, I'm going to graph it again. This time I will use a histogram, which is similar to a bar chart, but not the same thing.

Specifically, a bar chart is used when we are displaying discrete categories. So, above, we had 20 countries, and a single bar for each country.

A histogram is instead when we divide our data into 'groups' of similar values, which are called 'bins'. Thus, each 'bar' on a histogram represents the number of elements (in this case, countries) which fall in that bin. So, in this case, we can divide the GDP variable into bins.

As an example, we can divide GDP per head into 'bins' of US\$10000, and plot a histogram

```{r}
ggplot(GDP, aes(x=GDP2020))+geom_histogram(binwidth=10000)
```

As you can see, there are nearly 120 countries in the first bin, which you can also see runs from -5000 to 5000 dollars. Of course, we know there are no countries with negative GDP per head, but this is just an artefact of the binning process.

This sort of chart gives us something very different to a bar chart. Instead, it is really a **frequency distribution**, and it charts the frequency of values of a given variable - in this case GDP per head.

So, here we have some idea of the **distribution** of the variable. It's clearly skewed quite heavily, with some outliers at the high end.

Let's move on to summarize this variable with some numbers:

```{r}
descr(GDP,
  headings = FALSE, # remove headings
  stats = "common" # most common descriptive statistics
  
)
```

What we have here are the classic 'summary statistics' for 2020 GDP per head.

One thing missing - which admittedly is not very useful in this case, is the Mode, or the 'most frequent' value.

```{R}
# Create the function.
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Create the vector with numbers.
CLEAN_GDP <- na.omit(GDP$GDP2020)
v <- (CLEAN_GDP)

# Calculate the mode using the user function.
result <- getmode(v)
print(result)


```

There you have it, the mode is 1875.441.

One important thing to note here is that the word **average** is badly misused when talking about data. Specifically, the *mean*, *median*, and *mode* can all at one time or the other be referred to as the **average** value of a given variable. But, as you can see, they are all different numbers! The word 'average' is a very vague word to use in this context, so do be careful about when you use it. You should always default to using the specific term you are talking about.

A few other summary statistics are calculated for 2020 GDPph below

```{r}
summary(GDP$GDP2020)
IQR(GDP$GDP2020, na.rm=TRUE) 
print(max(GDP$GDP2020, na.rm=TRUE)-min(GDP$GDP2020, na.rm=TRUE))

```

What we have here now includes the range (173454.4) and the interquartile range (15944.94), which are both useful measures of how 'spread out' the data is.

Each of the summary numbers above has strengths and weaknesses when describing different types of data. Here, we will mainly talk about the sensistivity to outliers.

Specifically, the **median** and **interquartile range** are less sensitive to outliers as measures of central tendency and range respectively than their counterparts the **mean** and the **range**. So, that's why you commonly see the median used so frequently in these sorts of situations.

But, what do we mean by outliers? Let's look at this in the context of another graphical display, the **box plot**.

## Box Plots

The box plot (and the other displays below) are criminally underused in my opinion. Most of us calculate a box plot at school, and then never again. But, it's a really useful way to understand a data set.

Let's do a boxplot for the 2020 GDPph data:

```{r}
#ggplot(COVID, aes( y=CASES))+geom_boxplot()

bp <- ggplot(GDP, aes(x="", y=GDP2020))+geom_boxplot()
bp
```

A box plot presents a lot of information very efficiently. The **median** is the thick horizontal, the white rectangle is bounded by the **interquartile range**, and outliers are represented as dots.

We can see here that the data is very skewed, and there are some very significant outliers at the high end (no prizes for guessing which countries they are).

A weakness of a box plot is that it doesn't actually show the distribution of the data. One way to do this is to add what is called *jitter*, which actually plots the values of each case, with a 'jitter' to avoid overlapping identical values.

```{r}
bp + geom_jitter(color="black", size=0.4, alpha=0.9)

```

So, we can see here that most of the data is clustered towards the bottom of the range - i.e. very low GDPph. This is of course the same conclusion as the historgram allowed us to draw. But, the box plot also gives us plenty of *other* information about the variable.

Even so, while box plots are nice, they can also obscure important facts about the distribution. Adding the jitter helps, but there are other ways to display this type of data that make the distribution more obvious.

## Violin Plots and Ridge Plots

Here, I'm going to give you a set of *even more underused* data displays, which can show some powerful things about variables.

In order to do so, I am going to simulate some data, to make sure it has exactly the features I want to show.

First, let's create the data and use box plots to display it, such as you are already experienced with above.

```{r}

# create a dataset
data <- data.frame(
  name=c( rep("A",500), rep("B",500), rep("B",500), rep("C",20), rep('D', 100)  ),
  value=c( rnorm(500, 10, 5), rnorm(500, 13, 1), rnorm(500, 18, 1), rnorm(20, 25, 4), rnorm(100, 12, 1) )
)

# Plot
data %>%
  ggplot( aes(x=name, y=value, fill=name)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) 
    

```

That's all very well, but as we know, this can obsure the distribution. So, let's add jitter:

```{r}


# create a dataset
data <- data.frame(
  name=c( rep("A",500), rep("B",500), rep("B",500), rep("C",20), rep('D', 100)  ),
  value=c( rnorm(500, 10, 5), rnorm(500, 13, 1), rnorm(500, 18, 1), rnorm(20, 25, 4), rnorm(100, 12, 1) )
)

# Plot
data %>%
  ggplot( aes(x=name, y=value, fill=name)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) 

```

Well, look at that. B has what is called a *bimodal* distribution.

Lets look at what is called a **violin plot** to emphasize this:

```{r}

# create a dataset
data <- data.frame(
  name=c( rep("A",500), rep("B",500), rep("B",500), rep("C",20), rep('D', 100)  ),
  value=c( rnorm(500, 10, 5), rnorm(500, 13, 1), rnorm(500, 18, 1), rnorm(20, 25, 4), rnorm(100, 12, 1) )
)

# Plot
data %>%
  ggplot( aes(x=name, y=value, fill=name)) +
    geom_violin() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) 
    

```

Neat! You can see how the violin plot really emphasizes the differences in the distributions, which are totally obscured by the box plots, and only partially shown by adding the jitter.

Another way to visualize this is with **ridges**, which basically compare the distributions of variables together:

```{r}


# create a dataset
data <- data.frame(
  name=c( rep("A",500), rep("B",500), rep("B",500), rep("C",20), rep('D', 100)  ),
  value=c( rnorm(500, 10, 5), rnorm(500, 13, 1), rnorm(500, 18, 1), rnorm(20, 25, 4), rnorm(100, 12, 1) )
)

# Plot
data %>%
  ggplot( aes(x=value, y=name, fill=name)) +
    geom_density_ridges()
    

```

These are really cool in my view. And, if you are asking me, my preference in this type of situation would be to combine jittered box plots, which show the basics, and emphasize the outliers and summary numbers, with ridge plots, which show a really nice visualization of the distributions of each variable. Taken together, these plots allow us to understand a surprisingly large amount of important information about variables.

<!--chapter:end:01-intro.Rmd-->

# Associations and Relationships {#assoc_rel}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(readxl)
library(ggplot2)
library(psych)
library(ggpubr)

```

This chapter supplements the content in Lecture 5.

## Correlations and Associations

First, let's look at the basic concepts of correlation, using some data from the World Happiness Report, and GDP from the World Bank, all put together by the team at Our World in Data.

You can explore their work here: <https://ourworldindata.org/>

```{r}
Happy<-read_excel("Data/HappyGDP.xlsx", sheet = "2020")

```

First, I can just double check the data by describing it.

```{r}
describe(Happy)
```

We've got 249 countries, with population, GDP per capita, and Happiness. There's a fair bit of missing data, which you can see by looking at the numbers in the *n* column. That's ok, as long as we are aware of it.

Let's plot this data using a very basic scatterplot.

```{r}
plot(Happy$GDPpc, Happy$Happiness)
```

Interpretation: there's seemingly an association here. As one variable increases, the other does too.

It's worth noting that I have chosen to put GDPpc on the x-axis, which *implies* that the driver of Happiness is GDP. This replicates the way that most of the media articles that are regularly written on this kind of topic do it.

Nevertheless, this is just a scatterplot, so there is no real requirement to put any particular one of the variables on the x-axis, because it's important that you do **NOT** draw any causal conclusion from this very basic scatterplot. Simple associations are *not* in any way conclusive evidence of any causality in the relationship, since there are many other things which could be going on that are impossible to tease out simply with a bivariate association seen on a scatterplot. For example, there could be a spurious relationship - what could cause *both* happiness and GDPpc to increase?

Ideas? Could it simply be something like 'political stability'? Or something else?

Either way, all we have here is a basic data display. While, as I showed previously, simply displaying data can be very powerful, it is still only essentially an 'eyeball' analysis, and different people might draw different conclusions here (we'll discuss that later on).

What we want to do, is have some sort of objective **metric** for the relationship, and for this, we have the **correlation** statistic.

```{r}
Assoc1 <- cor.test(Happy$Happiness, Happy$GDPpc, 
                    method = "pearson")

#Assoc2 <- cor.test(Happy$Happiness, Happy$GDPpc_log, 
  #                  method = "pearson")
Assoc1
#Assoc2

```

Here, we can see that the correlation is 0.75

But what does that mean? We can **return to the slides for some explanation**.

## Introducing Nonlinear Associations

Let's return to the scatterplot of GDPpc and Happiness

```{r}
plot(Happy$GDPpc, Happy$Happiness)
```

As the *Economist* analysis discussed in the lectures suggests, this association also looks kind of *nonlinear*.

However, unlike the *Economist*, we can also say that there are multiple ways to look at this:

First, you could take the *Economist's* view that this is probably a **nonlinear relationship.** Indeed, we can check this out by *transforming* GDPpc and comparing the results between transformed and non-transformed data.

Let's have a try at that. What we will do is transform the GDPpc variable using a log transform (i.e. multiplying GDPpc by the natural logarithm).

```{r}
Happy$GDPpc_log <- log(Happy$GDPpc)
describe(Happy)
```

We can see there is a new variable now, GDPpc_log, which is the log transformed GDPpc variable - basically the original GDPpc variable multiplied by the natural logarithm.

Of course, there are many other possible transforms, and I'll touch on that below, for now let's rerun the plot using GDPpc_log

```{r}
y <- Happy$Happiness
x <- Happy$GDPpc_log
plot(x, y)
```

That relationship definitely looks more linear, and if that was my only criteria for deciding what to do with the data, I would definitely now run with this idea, as the *Economist* did in their article

With that in mind, let's take a quick look at the correlations for these variables. First the original GDPpc, and then the log transformed one.

```{r}
Assoc1 <- cor.test(Happy$Happiness, Happy$GDPpc, 
                    method = "pearson")

Assoc2 <- cor.test(Happy$Happiness, Happy$GDPpc_log, 
                    method = "pearson")
Assoc1
Assoc2

```

We can see that the association is stronger (the correlation is higher) for the log GDPpc variable, although to be honest, it is not a very big difference here, compared to other data sets that I have seen discussed in the media. That's interesting in itself (note we are using 2020 data, so maybe the pandemic has something to do with it?)

However, **transformations** of data are kind of a complicated topic though. In essence, what we are doing is changing the distribution of one (or more) of the variables, in a systematic way, in order to convert a non-linear relationship (which may be unable to be tested with methods like correlation and regression) into a linear one.

There are many different types of transformations available to you, and the one you choose depends on the form of the nonlinearity. A **log** transform converts an exponential (or similar) relation into a linear one. But, there are specific transforms for many different forms. Oftentimes, it's not always obvious what exactly to do, and people often default to transforming **any** nonlinear-looking relationship, or highly skewed variable, using log or maybe the quadratic (squaring). It's not really that simple though, and I think we need to be a bit more logical and theory-driven on **why** we transform variables.

In this case, I admit that the logic makes sense, that there is still a relationship between GDPpc and happiness at the top end, but you just have to increase GDPpc by a **lot more** to get the same change in happiness, compared to the change needed at the lower end of GDPpc. And, making that transform to come to that conclusion actually makes a difference to how we think of the influence of GDPpc on happiness, and could concievably lead to policy changes compared to the prior non-transformed analysis.

**Even so**, I have always had a slightly different possibility on my mind. In fact, it may be that there are **two groups** of country, low and high income, and different **linear** associations within those groups. That's a similar-ish idea to the nonlinear one, but it is a bit different, and I actually prefer that idea. A nice challenge would be to work out a way to test which of the two alternative explanations is actually better supported by the data...

Maybe I'll leave that for another day though.

## Regression

We can add a **regression line** to this scatterplot, for some extra information over the correlation.

```{r}
ggscatter(Happy, x = "GDPpc", y = "Happiness", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "GDP per capita", ylab = "Happiness")
```

We can also do this with the log GDPpc to compare.

```{r}
Happy_log<-Happy
ggscatter(Happy_log, x="GDPpc_log", y="Happiness" ,
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "log GDP per capita", ylab = "Happiness")
```

You can see that using the log GDPpc variable does improve things, but not by that much in my view. That said, it's clear that the line is a bit more 'precise' (the errors are lower which can be seen by the narrower shaded regions). So, if we are looking for the 'best' model, it's pretty clear that the log GDPpc model is better than the simple GDPpc model.

Either way, the regression line adds a layer of information on to the correlation, which allows us to predict *y* from *x*.

How so? Well, let's add the regression equation to the chart to see:

```{r}
Happy_log<-Happy
ggscatter(Happy_log, x="GDPpc_log", y="Happiness" ,
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "log GDP per capita", ylab = "Happiness")+
#add regression line code from https://rpkgs.datanovia.com/ggpubr/reference/stat_regline_equation.html
  
  stat_regline_equation(
  mapping = NULL,
  data = NULL,
  formula = y ~ x,
  label.x.npc = "center", #put in center not left (my own change)
  label.y.npc = "top",
  label.x = NULL,
  label.y = NULL,
  output.type = "expression",
  geom = "text",
  position = "identity",
  na.rm = FALSE,
  show.legend = NA,
  inherit.aes = TRUE
)

```

This adds an *intercept*, and with that plus the coefficient we have all that we need to plot a straight line, which we can *extrapolate* to higher values of GDPpc (although remember this is the logged GDP variable) and predict what the happiness scores would be.

This is obviously viable to the extent we can justify the relationship, and also within the parameters of our variables, and how confident we are that the relationship is consistent at all levels of the variables.

## Multiple Regression

Here, we will use a simple three-variable set of simulated data, which represents rates of smoking, rates of cycling, and heart disease incidence. This data is available from: <https://www.scribbr.com/statistics/linear-regression-in-r/>

Remember, this is **simulated data** not real data.

```{r}
Heart<-read_excel("Data/heart.data.xlsx")
describe(Heart)
```

Next, let's run three simple regressions among the three variables

```{r}
ggscatter(Heart, x = "smoking", y = "heart.disease", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Smoking", ylab = "Heart Disease")
```

```{r}
ggscatter(Heart, x = "biking", y = "heart.disease", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Biking", ylab = "Heart Disease")
```

```{r}
ggscatter(Heart, x = "smoking", y = "biking", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Smoking", ylab = "Biking")
```

Remember, these are *simple* regression lines, with bivariate scatterplots. What I mean, is the effect of biking on heart disease does not take account of the effect of smoking on heart disease. What we need to do is run a model which takes account of *both* the predictors, as **we can explain on the slide-deck...**

Now we can run the Multiple Regression Model

```{r}
heart.disease.lm<-lm(heart.disease ~ biking + smoking, data = Heart)

summary(heart.disease.lm)
```

We can interpret these results in the following way:

A 1 unit increase in biking will on average lead to a 0.2 unit decrease in heart disease A 1 unit increase in smoking will on average lead to a 0.17 unit increase in heart disease.

Of course, what these 'units' refer to depends on what you have measured them with of course. But, the unarguable interpretation is that the effect is strong.

However, because the *scales of the variables* are different, its not really possible to compare the sizes of the effects. So to some extent we don't know which of the two variables has the 'bigger' effect here, relatively at least.

To do that, we need to standardize the coeffiecients, which basically means putting all the variables on the **same scale**. Beforehand, they all were on different scales, which you can see by going back up and looking at the range, mean, and standard deviation of the original variables. You can see for yourself they are all different.

We can solve this issue by creating a new set of standardized data, and running the model on that, as follows:

```{r}
std_Heart = data.frame(scale(Heart))
describe(std_Heart)
```

You know these variables are standardized as they now all have a mean of 0 and a standard deviation of 1. Now, let's run our model on this standardized data set.

```{r}
std.heart.disease.lm<-lm(heart.disease ~ biking + smoking, data = std_Heart)

summary(std.heart.disease.lm)
```

You can see the things that have changed are the 'Estimates', which are now standardized.

In decimal, they are -0.94 for biking and 0.32 for Smoking.

The correct interpretation of these standardized effects is that for every 1SD increase in biking, you expect a 0.94SD decrease in heart disease, and for every 1SD increase in smoking, you would expect a 0.32 increase in heart disease.

So, we can see that actually biking has a much **higher** relative effect on heart disease than smoking (although in the opposite direction). That said, this assumes that both the two IVs have similar standard deviations, and distributions, and we didn't check that in this example.

## Visualizing Multiple Regression

There are loads of different ways to visualize multiple regression. It's not trivial, because we have more than two variables, so we can't use the techniques we used already.

Some people like to use 3D-style plots, which look cool, but are not always easy to interpret, and take quite a lot of extra work, for what I would say is not that much payoff (if any).

In this case, we can use a pretty simple visualization, where we could plot the relationship of biking to heart disease at different levels of smoking.

This would be a fairly typical way to do things if we thought the relationship between biking and heart disease changed according to the level of smoking, in which case it would be a moderator. Here, it doesn't really work that way, but it's a cool visualization regardless. It does require some data prep, but not that much, and I took the basic idea from the website where I sourced the data: <https://www.scribbr.com/statistics/linear-regression-in-r/>

```{r}
viz.data<-expand.grid(
  biking = seq(min(Heart$biking), max(Heart$biking), length.out=30),
    smoking=c(min(Heart$smoking), mean(Heart$smoking), max(Heart$smoking)))

viz.data$predicted.y <- predict.lm(heart.disease.lm, newdata=viz.data)

viz.data$smoking <- round(viz.data$smoking, digits = 2)

viz.data$smoking <- as.factor(viz.data$smoking)

heart.plot <- ggplot(Heart, aes(x=biking, y=heart.disease)) +
  geom_point()

heart.plot <- heart.plot +
  geom_line(data=viz.data, aes(x=biking, y=predicted.y, color=smoking), linewidth=1.25)

heart.plot
```

Here, we can see that the effect of smoking is really just to raise the likelihood of heart disease, however much biking you do.

So, for a given person who bikes a given amount, if they smoke more they will have a higher risk than a person who bikes the same amount but smokes less.

But, *for a given smoker*, the more they bike, the lower their risk of heart disease, to the extent that if a heavy smoker bikes enough, their actual risk of heart disease could even be lower than a non-smoker who does not bike at all.

Don't forget, this is not real data, but the point stands. Get on your bike.

<!--chapter:end:02-assoc_rel.Rmd-->

# Beginning to Understand Uncertainty {#uncertainty}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(readxl)
library(ggplot2)
library(boot)
library(rstatix)
library(skimr)
library (infer)
library(ggpubr)
library(dplyr)

```

This Chapter relates to lecture 6.

In this chapter I'll introduce core concepts around **uncertainty** in our results. Understanding that the results of our analysis always contain some level of uncertainty is probably the most critical concept to get our heads around as quantitative social scientists. Most of our job is not really coming up with the actual *statistics*, such as the correlation coefficient, or regression beta, but is more about understanding how to interpret and use those results - i.e. **what they mean**. And, fundamental to that is understanding their uncertainty.

Again, to reiterate the message I sent in class, many of the examples in this Chapter, and later ones, involve **randomness**. This means that the results here may be slightly different numerically to the results in the slides. And, if you were to run these examples yourself, you would also get slightly different results. This is nothing to worry about, because the meaning of the results does not change.

So, to start the journey, let's grab some data.

Here, we will again use the simple three-variable set of simulated data, which represents rates of smoking, rates of cycling, and heart disease incidence.

```{r}
Heart<-read_excel("Data/heart.data.xlsx")
head(Heart)
```

Rather than do the full 'describe' as I did in the last chapter, I have simply above looked at what is called the 'head' of the data set or the first few rows. This is because all I want to do here is double check that I have the data, and what variables are there.

Let's calculate some simple summary statistics from this data set to build on. For example, what is the mean and median for 'smoking'?

```{r}
summary(Heart$smoking)
```

Now, we know this is **really** simulated data, but let's **imagine** for now that it was actually obtained by an organization like the Office for National Statistics in the UK, using a survey. We can presume the study was done well, and thus it is based on a true random sampling method, and we assume that the study population matches whatever target population we have in mind (remember the 'inference gaps' discussed in class).

What we really want to know is, how close are these statistics (i.e. the mean and median) to the *true population values* that we would have found if we could survey the entire target population?

Let's begin to think about this by starting to build a table using these statistics, **by going back to the slide deck...**

## Demonstration: Sampling from a 'Known' Population

Now, let's go back **one more step**, and demonstrate the uncertainty inherent to sample statistics by way of example.

Let's *now* assume that this sample of 498 people actually *is* the population we are interested in.

What **this** means is, we can actually *draw a sample from this population of 498* and see what happens.

First, let's present the distribution for the entire 'population' of 498.

```{r}
ggplot(Heart, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

Now, let's literally take a *sample* of 10 random cases from that population of 498. Here, we are **sampling without replacement**, and are thus essentially doing exactly what a hypothetical 'researcher' would do if they drew a random sample of 10 people to complete their survey, from the population of 498.

```{r}
sub.10 <- Heart[sample(nrow(Heart), size = 10, replace = FALSE), ]
sub.10
```

Next, let's look at the relevant statistics (median and then mean) and distribution of this sample of 10:

```{r}
median(sub.10$smoking)
mean(sub.10$smoking)
```

```{r}
ggplot(sub.10, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

We can do the same for successively larger samples, say 50, and 200:

```{r}
sub.50 <- Heart[sample(nrow(Heart), size = 50, replace = FALSE), ]
sub.50
median(sub.50$smoking)
mean(sub.50$smoking)
ggplot(sub.50, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

```{r}
sub.200 <- Heart[sample(nrow(Heart), size = 200, replace = FALSE), ]
sub.200
median(sub.200$smoking)
median(sub.200$smoking)
ggplot(sub.200, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

As you can see, the distributions of the smaller samples are more peaky and bumpy, because they are very sensitive to individual data points. As the sample gets larger, it starts to look more like the population right?

We can complete our table now in the slides of the sample statistics (median and mean) showing that in general, as we get closer to the population size, the statistics generally get closer too. To do so, let's go **back to the slides...**

<!--chapter:end:03-estimation.Rmd-->

# Introduction to Bootstrapping {#bootstrap}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(readxl)
library(ggplot2)
library(boot)
library(rstatix)
library(skimr)
library (infer)
library(ggpubr)
library(dplyr)

```

This Chapter supplements the material in the first part of Lecture 7.

In this chapter, we will learn the core concepts of bootstrapping. That is, creating synthetic sampling distributions through multiple resampling (with replacement) of a single sample.

The basic process is fairly simple, once you have your original sample, and has the following characteristics:

1.  A bootstrap sample has an equal probability of randomly drawing any of the original sample elements (data points).
2.  Each element can be selected more than once - because the sample is done **with replacement**.
3.  Each resampled data set (the new sample) is the same size as the original one.

First, I will demonstrate the basic principle.

Recall from the last chapter that there was a simulated data set of 498 people, with variables representing smoking, biking, and heart disease.

We treated this as **the population** and then sampled from it to demonstrate uncertainty at different sample sizes.

So, let's take the sample of 50 that we drew from that 'population' of 498, and imagine that we got this sample by (for example) doing a survey of the population (of 498), and this is our data set for analysis.

First we will remind ourselves of the properties and distribution of our sample, the median, mean, and distribution:

```{r}
median(sub.50$smoking)
mean(sub.50$smoking)
ggplot(sub.50, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")
```

Ok, there we go. Remember, this sample is our 'data set' for analysis.

We know the median and mean, but we have no real indication of the uncertainty in those estimates. In order to get that information, we will eventually use the bootstrap method. For now, we are just going to demonstrate the basic idea.

So, what we do now is draw *another random sample of 50* from this 50, but each time we draw a data point, we *replace* it back, so we are always drawing our sample from the full 50. This is called *sampling with replacement*.

In this way, the new sample can **only contain values which were in the original sample**, but can contain different frequencies of those values. Or in other words, each value can occur many different times, and that number of times may be different to the original sample. So the *distribution of values* in this new sample will be different to the original sample, and the statistics will therefore also be different.

Let's draw this new sample, take the median and mean of the sample, and plot it:

```{r}
Boot.1 <- sub.50[sample(nrow(sub.50), size = 50, replace = TRUE), ]
Boot.1
median(Boot.1$smoking)
mean(Boot.1$smoking)
ggplot(Boot.1, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

Marvelous! Now, for the purpose of example, let's draw two more of these resamples from the original 50, take their median and mean, and plot the distributions...

```{r}
Boot.2 <- sub.50[sample(nrow(sub.50), size = 50, replace = TRUE), ]
Boot.2
median(Boot.2$smoking)
mean(Boot.2$smoking)
ggplot(Boot.2, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

```{r}
Boot.3 <- sub.50[sample(nrow(sub.50), size = 50, replace = TRUE), ]
Boot.3
median(Boot.3$smoking)
mean(Boot.3$smoking)
ggplot(Boot.3, aes(x=smoking))+geom_histogram(aes (y=..density..), colour="black", fill="white")

```

Now, if we **return to the slides**, we can build a table using these mean and median values. Of course, the slide deck will have slightly different values, since it's based on a different run of the resampling process, but the principle is the same.

So, this is the basic idea of bootstrapping. We sample with replacement from our original sample, many many times. We did 3 here manually, but we generally use a program to do this many more times, such as a thousand or more.

## Bootstrapping in the Context of Previous Examples

To further reinforce the point, let's now place ourselves in the position of three different researchers, each of varying levels of enthusiasm, and all three are researching the **same population of 498 people** that we have already explored in the last few examples.

Researcher 1 is a little like me as a Ph.D. student, and maybe more interested in 'experiencing life'. So, he has little time to actually collect data, and not much more enthusiasm for it. In the end, he manages to take a sample of 10 people from the population of 498.

Researcher 2 is a bit more enthusiastic, and gets a sample of 50.

Researcher 3 is fairly conscientious, and takes a sample of 200 from the population of 498.

Now, what we can do, is run 1000 bootstrap replications of each of these varying-sized subsamples of the population, to see what might happen:

First, the 10:

```{r}

f1 <- function(data, i){
  d2<-data[i,]
  return(mean(d2$smoking))
}

# bootstrapping with 1000 replications
set.seed(1234)
results <- boot(data=sub.10, f1,
   R=1000)

# view results
results
plot(results)

# get 95% confidence interval
boot.ci(results, type="norm")

```

Now let's do it for the other two subsamples of n=50, and n=200

```{r}

f1 <- function(data, i){
  d2<-data[i,]
  return(mean(d2$smoking))
}

# bootstrapping with 1000 replications
set.seed(1234)
results <- boot(data=sub.50, f1,
   R=1000)

# view results
results
plot(results)

# get 95% confidence interval
boot.ci(results, type="norm")

```

```{r}

f1 <- function(data, i){
  d2<-data[i,]
  return(mean(d2$smoking))
}

# bootstrapping with 1000 replications
set.seed(1234)
results <- boot(data=sub.200, f1,
   R=1000)

# view results
results
plot(results)

# get 95% confidence interval
boot.ci(results, type="norm")


```

We'll now build a table with these values *back in the slide deck. I*Again, remember the values in the slide deck will differ from these due to the randomness of the process.

Now, let's shift our minds a bit, and consider that the data set of 498 actually represents **a sample of a larger population** (remember from the last chapter, it's simulated, but meant to represent a sample from the population).

So, let's bring in Researcher 4, the most conscientious of all. She is the one who manages to take a sample of 498 people from the population. And, *finally*, we can bootstrap the original full sample of 498:

```{r}

f1 <- function(data, i){
  d2<-data[i,]
  return(mean(d2$smoking))
}

# bootstrapping with 1000 replications
set.seed(1234)
results <- boot(data=Heart, f1,
   R=1000)

# view results
results
plot(results)

# get 95% confidence interval
boot.ci(results, type="norm")

```

This is a very nice set of results, which can tell us many interesting things. *So let's go back to the slides.....*

## Bootstrapping Other Stuff...

We have so far only bootstrapped the mean. However, the basic principle can be applied to virtually any statistical estimate. So, we can revisit some of our prior analyses, and use the bootstrap method to quantify the uncertainty in the estimates that we previously accepted without really thinking too hard about them.

### Correlations

First, let's revisit our recent correlation analysis of Happiness and GDP per capita.

```{r}
Happy<-read_excel("Data/HappyGDP.xlsx", sheet = "2020")

head(Happy)
describe(Happy)

```

If we run the same analysis as in Chapter 2, we'll get the same results: Correlation R = 0.75

```{r}
ggscatter(Happy, x = "GDPpc", y = "Happiness", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "GDP per capita", ylab = "Happiness")

```

Now, let's take uncertainty into account, by bootstrapping that correlation and creating some confidence intervals.

```{r}
#Using Infer to bootstrap statistics. See link below for info
#https://cran.r-project.org/web/packages/infer/vignettes/observed_stat_examples.html

#calculate correlation
corr_hat <- Happy %>% 
  specify(Happiness ~ GDPpc) %>%
  calculate(stat = "correlation")

#generate the bootstrap distribution
#make sure to set the seed for replicability
set.seed(1)
boot_dist <- Happy %>%
   specify(Happiness ~ GDPpc) %>% 
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "correlation")

#use bootstrap to find confidence interval
percentile_ci <- get_ci(boot_dist)

#visualize this - so cool!
visualize(boot_dist) +
  shade_confidence_interval(endpoints = percentile_ci)

#report the actual numbers for info
percentile_ci
corr_hat

```

So, you can see the correlation is 0.75 with a 95% confidence interval of 0.69 - 0.81

Now, let's extend this to the multiple regression case we have previously used, examining the relationships between smoking, biking, and heart disease.

```{r}
Heart<-read_excel("Data/heart.data.xlsx")

head(Heart)
describe(Heart)
```

Here, we need to calculate multiple confidence intervals as we have multiple estimates.

```{r}
#Using Infer to bootstrap statistics. See link below for info
#https://cran.r-project.org/web/packages/infer/vignettes/observed_stat_examples.html
#note, used bootstrap here not permutation as in the original code

#calculate the regression model fit
obs_fit <- Heart %>%
  specify(heart.disease ~ smoking + biking) %>%
  fit()

#create bootstrap distribution
#make sure to set the seed for replicability
set.seed(1)
null_dist <- Heart %>%
  specify(heart.disease ~ smoking + biking) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  fit()

#find confidence intervals
conf_ints <- 
  get_confidence_interval(
    null_dist, 
    level = .95, 
    point_estimate = obs_fit
  )

#visualize the results
visualize(null_dist) +
  shade_confidence_interval(endpoints = conf_ints)

#report actual numbers
obs_fit
conf_ints

```

It's worth reflecting on exactly what these conflidence intervals mean, and to do so, we can move **back to the slides...**

<!--chapter:end:04-bootstrapping.Rmd-->

# T-Tests for Means {#t-test}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
library(readxl)
library(ggplot2)
library(boot)
library(rstatix)
library(skimr)
library (infer)
library(ggpubr)
library(dplyr)

```

This Chapter supplements the material in the second part of Lecture 7.

In this chapter, I will introduce the t-test, a classic analysis technique that is used to compare two groups together.

## Independent Sample T-Tests

We can also use bootstrapping as an entry point to a new analysis situation, where we are comparing two groups. This could be for example in a classic experimental context; treatment and control.

Remember, t-tests can be done in any analysis setting, and it does not require bootstrapping at all. It just so happens that they are nicely explainable at this point.

So, we are going to analyze a set of data from my infamous Ed Sheeran Study^1^ - which would certainly win an Ignobel Prize if I were ever to do it in reality rather than in my fondest imaginings.

The design was discussed in the lecture slides, but in brief we have a two-group 'treatment and control' experiment, the classic 'randomized control trial' style study.

The first thing to do is read in the data and describe it.

```{r}
ED_IND<-read_excel("Data/SHEERAN_T.xlsx", sheet = "IND")
```

```{r}
head(ED_IND)
describe(ED_IND)
ED_IND$GROUP <- factor(ED_IND$GROUP)
skim(ED_IND)
```

We can see that we have one FACTOR variable, which we need to indicate the group each subject was in (CONTROL being GROUP 1, and ED being GROUP 2).

So, let's run an *independent samples T-Test* with bootstrapped confidence interval.

We use an independent samples test, as the *theory* is these two groups - after participating in the experiment - are no longer the 'same'. There is something 'different' about the population of people who listen to Ed Sheeran, compared to those who do not. What we are trying to do is assess whether this difference is in *anger.*

```{r}

#First, some code to display the group means using dplyr
ED_IND %>%   # Specify data frame
group_by(GROUP) %>% # Specify group indicator
summarise_at(vars(ANGER), # Specify column
list(name = mean))    # Specify function

#use infer to bootstrap the t-value
#https://infer.netlify.app/articles/observed_stat_examples

t_hat <- ED_IND %>%
  specify(ANGER ~ GROUP) %>%
  calculate(stat = "t", order = c("1", "2"))

boot_dist <- ED_IND %>%
   specify(ANGER ~ GROUP) %>%
   generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "t", order = c("1", "2"))

percentile_ci <- get_ci(boot_dist)


visualize(boot_dist) +
  shade_confidence_interval(endpoints = percentile_ci)

#display the t and CI
t_hat
percentile_ci

```

Remember: Group 1 is the control, and Group 2 listened to Ed Sheeran.

Cool, so it seems that Group 2 displayed more anger. The confidence interval for the t-statistic does not contain 0, so it supports the idea that there is a difference here. The interval is quite wide though - because of our small sample size. In a later Chapter we'll return to this issue of what confidence intervals actually mean.

## Paired-Sample T-tests

OK, so let's use a different design, using a paired samples t-test. Let me go *back to the slides...* to explain this difference.

The same basic process is needed, but with some modifications because of the type of comparison we are doing. And, as such, we have some new data.

```{r}
ED_PAIR<-read_excel("Data/SHEERAN_T.xlsx", sheet = "PAIR")
ED_PAIR <- data.frame(ED_PAIR)
head(ED_PAIR)
describe(ED_PAIR)
```

Now, for this bootstrap purpose we actually need to to calculate the **difference between the two measurements** (here, T1 and T2).

Then, we bootstrap a one-sample t-test with this difference variable. This essentially tests whether the difference is different from zero. If it is, that indicates some effect of the treatment.

Let us first calculate the new variable:

```{r}
ED_PAIR$DIF <- ED_PAIR$ANG_T2-ED_PAIR$ANG_T1
head(ED_PAIR)
describe(ED_PAIR)
```

We can see there is a new 'DIF' variable here.

Next, we bootstrap a confidence interval for the *mean of the difference variable*, to see whether it includes zero:

```{r}
#modified from https://infer.netlify.app/articles/observed_stat_examples.html


x_bar <- ED_PAIR %>% 
  specify(response = DIF) %>%
  calculate(stat = "mean")

boot_dist <- ED_PAIR %>%
   specify(response = DIF) %>%
     generate(reps = 1000, type = "bootstrap") %>%
   calculate(stat = "mean")

percentile_ci <- get_ci(boot_dist)

visualize(boot_dist) +
  shade_confidence_interval(endpoints = percentile_ci)

#report the results
x_bar
percentile_ci
mean(ED_PAIR$ANG_T1)
mean(ED_PAIR$ANG_T2)


```

Marvelous. We can see that the results suggest that after listening to Ed Sheeran, our sample on average reported more anger. This is because the 95% confidence interval does not include 0, and therefore I am confident in saying that there is probably some effect going on here.

Not surprised...

^1^If you are a friend or relative of, or more importantly a lawyer for, Ed Sheeran, please note that this is not a real study.

<!--chapter:end:05-t-test.Rmd-->

# Introduction to Probability {#probability}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)
```

This Chapter provides examples and additional content for Lesson 8.

In this lesson, we will learn about the classical theory of probability, and how it can be used to construct measures of uncertainy of our estimates, and from there we can begin to draw inferences from the sample to the population.

## Simulating Coin Flips

The first thing we'll do is simulate 5000 fair coin flips.

Remember, this is another random process, so results will differ each time slightly

```{r}
#Note: This is virtually copied verbatim from: <https://rpubs.com/pgrosse/545948>
#simulate 5000 flips of a fair coin
#create data frame for trial number and outcome of an individual coin flip
flips <- sample(c(0, 1), 5000, replace = TRUE)
flips <- matrix(flips, ncol = 1)
flips <- as.data.frame(flips)
Trial <- seq(1, 5000, 1)
Trial <- as.data.frame(Trial)
flipsim <- cbind(flips, Trial)
colnames(flipsim) <- c("Heads", "Trial")
```

```{r}
#calculate cumulative heads at the end of each trial
flipsim[,"Cum_Heads"] <- cumsum(flipsim$Heads)
flipsim <- flipsim %>% mutate(Pct_Heads = Cum_Heads/Trial)
head(flipsim)
```

Below, we'll plot the results of this simulation for ease of interpretation

```{r}
#create plot
fair_plot <- flipsim %>% ggplot(aes(y = Pct_Heads, x = Trial)) + ggtitle("Percentage of Heads \n Fair Coin") + geom_line() 

#note the below code creates the animation, but it does not look quite correct to me when it animates so I do not use it here. If you want to use this, copy the code after the hashtag (starting with a +) to the line above, just after the geom_line() 

#+ geom_segment(aes(xend = 5000, yend = Pct_Heads), linetype = 2,color = "red") + geom_point(size = 2) + transition_reveal(Trial) + ylim(0,1) + coord_cartesian(clip = "off") + theme(plot.title = element_text(hjust = 0.5)) 

fair_plot
```

The thing to notice (in combination with the table of results above the plot) is that eventually the proportion of heads versus tails approaches 50%, *but it does not start out that way*.

In other words, for a fair coin toss sequence, the trials (tosses) are *independent*, which means that for any given toss of the coin, the chance of a head or tail is 50%. But, that's not the same as saying that you'll get equal numbers of heads or tails for every (or any) particular sequence of multiple tosses - it's not 'self-correcting' in any other way than sheer weight of numbers. So, **eventually** over enough trials, the sequence should start to look like 50/50 heads and tails. But, that is not the tosses 'self-correcting', simply that you are tossing a lot of coins. This is at the root of a number of fallacies about predicting random events, such as the Gambler's fallacy.

## Premier League Goals and the Poisson Distribution

Note, much of the inspiration and code for this section comes from: <https://bookdown.org/theqdata/honors_thesis/goal-scoring-and-the-poisson-process.html>

The data comes from: <https://www.football-data.co.uk/englandm.php> and is the EPL results for 21-22 Season

```{r}
EPL<-read_excel("D:/Dropbox/R_Files/Data/EPL21-22.xlsx")
head(EPL)
```

Next, let's see a few important bits of descriptive data. First, let's calculate the total number of games in a season:

```{r}
summary(EPL$HomeTeam)

```

Next, how many goals in the season were there?

```{r}
sum(EPL$TOTG)
```

And, what was the mean number of goals per game?

```{r}
summary(EPL$TOTG)
```

OK, so now we have some idea of the goals per game, let's plot the distribution of goals.

```{r}
ggplot(EPL, aes(x = TOTG)) +
  geom_histogram(color = "darkgreen", fill = "lightgreen", bins = 10) +
  scale_x_continuous(breaks= 0:9)
```

If you've ever seen a Poisson distribution, you'll recognise this!

Let's check this out. First, let's create a table of all the matches with different numbers of goals.

```{r}
GoalsTable <- 
  EPL %>% 
  group_by(TOTG) %>% 
  summarise(ActualMatches = n())
GoalsTable 
```

Let's pull some stats of the Total Goals variable which well help us in a second.

```{r}
fav_stats(EPL$TOTG)
```

```{r}
#Below is some code which will help us to create the distributions and also check them.
MeanGoals <- fav_stats(EPL$TOTG)[[6]]
numMatches <- fav_stats(EPL$TOTG)[[8]]
StDevGoals <- fav_stats(EPL$TOTG)[[7]]
VarianceGoals <- StDevGoals ^ 2
```

Importantly, because the Poisson distribution is described only by its mean, the first and most basic check we can do is whether the mean and the variance of the variable are the same (or at least very close). We already have the mean above (2.82), but what is the variance?

```{r}
VarianceGoals
```

This is not exactly the same, but good enough to be going on with.

So, it seems to me at least that we can use the Poisson distribution to at least somewhat accurately describe the probability of occurance of a given number of total goals scored in a premier league game, for this season at least.

The first thing I am going to do with that information is build a figure which compares the *actual* numbers of goals scored in games with the *predicted* numbers which would be scored if the goals scored were a perfect Poisson distribution.

Ideally, we would build the Poisson probabilities for 0-9 goals (which is the maximum number scored in 21-22). However, if you do this, you will find that the two tables (actual and predicted goals) will have different numbers of rows. This is because there were no games with 8 goals scored in 21-22, but one with 9.

This is a bit annoying, but not a big issue. What we do is build a Poisson probability distribution for 0-8 goals, and treat the final probability as that for '8 or more' goals, for the purposes of drawing our figure. This isn't strictly correct, because we have not collected up our actual goal data into that category, but we could do that if we wanted to.

Later, we'll break this down properly, but for our showy graph here we don't really need to do it.

```{r}
PoisProb <- dpois(c(0:8), MeanGoals)
POIS <-data.frame(PoisProb)
POIS
```

So, this table of probabilities, based on the Poisson distribution, allows us to make some predictions of what we could expect.

Remembering again that there are 380 games in a season, we can see that there is a 0.16% chance of seeing 4 goals in a game, which equates to 380 x 0.16 = 60.8 games we would expect to see in a season with 4 goals.

We can check this out by creating a new table comparing **actual** with **predicted** values...

```{r}
NewGoalsTable <- cbind(GoalsTable, PoisProb) 
NewGoalsTable <- mutate(GoalsTable, 
                        ExpectedMatches = round(numMatches * PoisProb))
NewGoalsTable
```

Remember, as you can see above, the row for '8' is missing, and it goes straight to 9. So, to repeat, just take (for simplicity's sake here) the final row of Expected Matches as meaning '2 games with 8 or more goals'

Anyway, it's pretty scarily close! E.g. in 21-22 there were 60 matches with 4 goals, and that is identical to the predicted amount (the difference with the predicted 60.8 from the probability distribution is likely a rounding error).

Let's plot this for the big payoff:

```{r}
NewGoalsTable %>% 
  gather(ActualMatches, ExpectedMatches, 
         key = "Type", value = "numMatches") %>% 
  ggplot(aes(x = TOTG, y = numMatches, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge")
```

Wow, that is pretty accurate! Remember, the 'expected' figures are purely drawn from the *entirely theoretical* Poisson distribution with a mean of 2.82. They very closely match the actual data.

To be fair though, that graph is really just a bit of show, but what we want is the 'go'. In other words, what could we do with this information?

Well, there is a lot of money spent on sports betting. We could use this information to help us decide whether we should place a bet that **there will be a game in the 22-23 season with 10 goals?** - assuming that this is being written before the 22-23 season (which is true).

Remember, there were 380 games in 21-22, none of which contained 10 goals. So, for ease of thinking about this, let's make the assumption that the PL started in 2021, and there were no games before to count (in reality, we would go back to the last time 10 goals were scored and count from there, but let's go with this in the first instance, and we'll expand later on, promise).

So, first, we need to calculate a new set of probabilities out to 10 goals.

```{r}
PoisProb <- dpois(c(0:11), MeanGoals)
POIS <-data.frame(PoisProb)
POIS
```

So, the number we want is the second-last probability: 0.00052.

This is because the table starts from the probability of 0 goals.

This means we can expect a game with 10 goals to happen every 1923 games

Calculate this by dividing 1 by the probability.

So, given there are 380 games per season, you would expect a game with 10 goals to happen once every 5 seasons.

So, if I started counting from the 21-22 season, the answer is NO, I would not bet on there being a game with 10 goals in the 22-23 Premier League season.

But, let's add some (pretty obvious) further context, as of November 2022, there have been 5 games in the history of the Premier League where 10 goals have been scored (and 1 with 11).

The Premier League has been going since 1992, and so far in Nov 22 there have been...30 seasons. So, we are probably due one.

AND, the last game with 10 goals was in 2013 (Man Utd 5, West Brom 5)

So, I reckon we are *definitely-maybe-probably* due one.

By the way, as of the start of the 22-23 season, there had been 21 matches with 9 goals. We would expect given our Poisson distribution that a game with 9 goals should happen every 1.4 seasons, meaning over 30 seasons we would expect.... 21.

### 2023 Update

So, the above example is *totally unchanged* (other than some typo corrections and nicer wording) from when I originally coded it in 2022. The million dollar question: *was there a game with 10 goals in the 22-23 season...?*

The answer is no, there wasn't. But there were two with 9! Luckily I am not a betting man. I do think we are due one, so this season (2024) is probably a good bet, but as of January 20th 2024, we still haven't had one... Unless of course you think something has fundamentally changed about the premier league in recent years, changing the probability of goals (and thus suggesting recent seasons are drawn from a *Poisson distribution with different characteristics* than past ones), which is in fact an interesting question in its own right...

## Birth Weight and the Normal Distribution: Brief Example

Let's consider UK births in 2020, since they're a subject close to my heart, as my son was born that year.

Looking online, one can pull down the distribution of UK birthweights in 2020, available from the UK ONS website, and which are reported in 500g 'bins'.

Source: <https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/birthsummarytablesenglandandwales/2020#births-data>

We can use these numbers to create a chart, and see what it looks like, inspired by David Spiegelhalter's originals, which are available on his Github page.

Spiegelhalter's Github can be found here: <https://github.com/dspiegel29/ArtofStatistics>

```{r}
weights=c(1500, 2000, 2500, 3000, 3500, 4000, 4500,5000,5500) #modified categories from 2020 UK Data
mids=weights-250 # Note the DS original is +250, but this does not work for UK data as the numbered is the UPPER bound of the weight category in the ONS stats. So the babies in the '2000' bin weigh 1500-1999, so the midpoint is 1750, not 2250
n=c(5015, 7103, 27554, 99220, 218442, 179088, 53530,6952,635) # numbers in each bin, UK2020
N=sum(n)  # total number of babies
area=N*500  # number * binwidth = total area of histogram

#I think lbw should be sum of groups 1-3 not 1-2 as in the book code. Using 1-3 gives a result for the UK of 6.6% which tallies with the official stat given that I remove the 'no recorded weight' and 'implausible' categories'. I imagine that the DS book data has categories which may be the lower bound not the upper as in the UK data table.

lbw   = sum(n[1:3])   # number with low birth weight (less than 2500)
lbw.percent=100*lbw/N  # % low birth weight
# 6.6% which tallies with Nuffield stat for 2020.

#calculate mean and sd of population
# could use sheppard's correction

birth.mean=sum(n*mids/N)
birth.sd=sqrt( sum(n*(mids-birth.mean)^2)/N)



par(mfrow=c(2,2))
# setup plot ranges noting max of normal density is at mean
xrange <- c(1500,6000)
yrange <- range( c(n, area*dnorm(birth.mean, birth.mean, birth.sd), 0))
scale=0.6
par(mar=c(5,0,1,0)+0.1)

# (a) empirical distribution and fitted normal
plot(xrange, yrange, type = "n", xlab = "", ylab = "",
     bty="n",axes=F,main="2020 UK Birthweights(g)", cex=scale)
axis(1,cex=scale) 
# draw bars using rect and density using curve
rect(weights, 0, weights - 500, n, col = "lightblue")
curve(area*dnorm(x, birth.mean, birth.sd), min(xrange), max(xrange), add = TRUE, 
      lwd=3, col="blue")

print (birth.mean)
print(birth.sd)

```

The first number is the mean, the second is the standard deviation.

What you can see here is, just using the real data from 2020 UK birthweights, it looks very much like what is called a 'normal distribution', or what you might have heard called a 'bell curve'. We can almost certainly treat any given child's birthweight as if it was a random variable drawn from a normal distribution, with a mean and standard deviation that is given by the empirical data here (3345.263 and 577.7594 respectively).

<!--chapter:end:06-probability.Rmd-->

# Introduction to Statistics {#statistics}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)

library(TeachingDemos)
```

This Chapter builds on the material in Lesson 9.

Here, we begin to apply our probability concepts to the task of statistical inference in the 'classical' way.

The theory explained and demonstrated here is the foundation of the (very large) majority of the quantitative business and management research you will read and use. Almost any time someone refers to a finding being **(statistically) significant** or a **p-value**, they are using this idea. However, a major weakness in many social science fields - business and management being one - is that a rather large proportion of researchers don't actually understand the core principles of statistical inference, and are instead often just copying what other people have written, or told them to do.

So, if you can get your head around the next 3-4 chapters, you'll be in a strong position to work with what's already out there, and also do your own solid research.

## The Distribution of Sample Means, and the Central Limit Theorem

The first thing to cover is an extension of the ideas in the last chapter about sampling distributions. It's called the *Central Limit Theorem*, and is often the source of much angst in statistics students (including myself). However, here I am actually going to demonstrate how it works, and show how is actually quite amazing.

To do so, I will use a synthetic data set of monthly incomes from 20 people. Let us treat this data set as the **population**.

```{r}
CLT<-read_excel("D:/Dropbox/R_Files/Data/CLT.xlsx")
head(CLT)
summary(CLT$Income)
sd(CLT$Income)
```

Above, you'll see the important descriptive statistics, including the standard deviation of 2366.432. Let's plot the frequency distribution, and once we do so we will find that it is essentially uniform - every value occurs once and only once in the population.

```{r}
ggplot(CLT, aes(x=Income))+geom_histogram(binwidth=400, colour="black", fill="white")
```

Now, what I am going to do is take a *single random sample of 2* from that population. This sample is taken 'without replacement', if you are wondering, and so it is in essence exactly as if a researcher went into the field and took a sample from this population.

```{r}

vec <- CLT$Income
  
# generating 1 random combination of 2 of the 
# Income values 
print ("One Random Combination of 2 of the 20 Income Values")
res1<- sample(vec,2,replace=FALSE)
print (res1)


```

Let's take the mean of the sample:

```{r}
data1<-data.frame(res1)

head(data1)

mean(res1)
```

The mean of the random sample of 2 will of course be different each time the sample is taken (e.g. when writing this text, the mean was 10600, but this may not be the same in the version you are reading!)

Just for reference, the mean of the population (remember, the 20 people) was 9400

So, let's now do that for **every possible combination of 2 values** from this population.

Given there are n=20 values in the population, there are k=190 possible combinations of two values.

Note: This is not the same as bootstrapping - we are *not sampling with replacement* here. We are instead taking combinations. The thing to think about is that this is the equivalent of taking every single possible sample of 2 that you could take from this population.

Remember the ping-pong balls!

Let's do it:

```{r}
##code modified from: https://www.geeksforgeeks.org/calculate-combinations-and-permutations-in-r/
vec <- CLT$Income
  
# generating combinations of the 
# Income values taking 2 at a time
print ("Every Possible Combination of the 20 Income Values")

res<- combinations(n= 20, r = 2, v = vec)
print (res)
  
print ("Number of combinations without repetition")

print (nrow(res))
```

That was a rather long exhibit up there, but I wanted to show you that this was actually really done properly, and we have 190 combinations.

Next, let us create the means of each of the 190 combinations

```{r}
data<-data.frame(res)
data$MDIST <- rowMeans(data)

head(data)
```

That's just the first 6 (the head of the data set), but you can rest assured that there are 190 means created here.

OK, so here is the kicker. Let's plot a histogram of these 190 means.

Remember, a histogram is a frequency distribution:

```{r}
ggplot(data, aes(x=MDIST))+geom_histogram(bins=13, colour="black", fill="white")

```

Well well well!

Even though the original population was a completely uniform distribution with a mean of 9400, the distribution of all of the sample means looks quite a lot like a normal distribution / bell curve!

Let's overlay one on it as well to make the point...

```{r}
ggplot(data, aes(x=MDIST))+geom_histogram(bins = 13, aes (y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")
```

Further, let's take the mean of those means...

```{r}
summary(data$MDIST)
```

Hello! It turns out, the mean of those means is the *population mean!!!*

This, in a demonstration, is *the central limit theorem.*

Let's **move back to the slides** to talk about this some more.

## Demonstrating Confidence Intervals {#conf}

Below I will demonstrate the idea of confidence intervals as a measure of precision, and at the same time help to really lock in the correct interpretation of what a confidence interval really means.

To do so, I will continue the in-class example of WBS Ph.D. graduate salaries. However, here, I will **simulate** the population distribution that we have sampled from. Let's recap:

Sample size (N) = 42

Sample mean = 170,000

Sample SD = 46617.4

I will simulate a population distribution that this sample could quite conceivably have come from. It will have the following features:

Population mean = 165,000

Population SD = 40000

I will also assume that the population distribution is *normal*. That is, the distribution of all business and management Ph.D. starting salaries is shaped like a bell curve - the normal distribution (remember birth weights from the last chapter). This is a reasonable assumption in my view, given the features of the phenomena we are investigating.

The important thing to remember is not really the population distribution of salaries (which is just used here to simulate the population), but the *sampling distribution of the means*. Remember, the central limit theorem shows us that **whatever the shape of the population distribution we are sampling from**, the sampling distribution of the means will take the form of a normal distribution, with a mean of the population mean.

So, below, I will simulate the population, and then calculate 100 confidence intervals for 4 different sample sizes (5, 10, 42, 100), to demonstrate two things:

a)  How confidence intervals get more precise as sample size increases, and
b)  How to interpret your specific confidence interval as simply one possibility of many.

```{R}

ci.examp <- function(mean.sim=100, sd=10, n=25, reps=50,
                     conf.level=0.95, method="z",
                     lower.conf=(1-conf.level)/2,
                     upper.conf=1-(1-conf.level)/2,
                     XLIM = 100 + 40*c(-1,1)) 
  {
data <- matrix( rnorm( n*reps, mean.sim, sd), ncol=n)
  rmeans <- rowMeans(data)
  switch(method, Z=,z={
    lower <- qnorm( lower.conf, rmeans, sd/sqrt(n))
    upper <- qnorm( upper.conf, rmeans, sd/sqrt(n))
  },
         T=,t= {
           cv.l <- qt(lower.conf, n-1)
           cv.u <- qt(upper.conf, n-1)
           rsds <- sqrt( apply(data,1,var) )/sqrt(n)

           lower <- rmeans+cv.l*rsds
           upper <- rmeans+cv.u*rsds
         },
         BOTH=, Both=, both={
           lz <- qnorm( lower.conf, rmeans, sd/sqrt(n))
           uz <- qnorm( upper.conf, rmeans, sd/sqrt(n))

           cv.l <- qt(lower.conf, n-1)
           cv.u <- qt(upper.conf, n-1)
           rsds <- sqrt( apply(data,1,var) )/sqrt(n)

           lt <- rmeans+cv.l*rsds
           ut <- rmeans+cv.u*rsds

           lower <- c(rbind(lt,lz,mean.sim))
           upper <- c(rbind(ut,uz,mean.sim))

           reps <- reps*3
           rmeans <- rep(rmeans, each=3)
           rmeans[c(F,F,T)] <- NA

         },
         stop("method must be z, t, or both") )

  if( any( upper==Inf ) ) upper <- rep( 2*mean.sim-min(lower), reps )
  if( any( lower==-Inf ) ) lower <- rep( 2*mean.sim-max(upper), reps )

  xr <- range( upper, lower )

  plot(lower,seq(1,reps), type="n",
       xlim=XLIM,       ## Changed
       xlab="Confidence Interval",
       ylab="Index",)

  ## abline( v= qnorm(c(1-upper.conf,1-lower.conf), mean.sim, sd/sqrt(n)), col=10) ## Deleted

  title(paste("Sample size is", n, "each"))     ## Changed

  colr <- ifelse( lower > mean.sim, 5, ifelse( upper < mean.sim, 6, 1) )

  abline(v=mean.sim)

  for( i in seq(1,reps) ){

    segments(lower[i], i, upper[i], i, col=colr[i])

  }

  points( rmeans, seq(along=rmeans), pch="|" )
  invisible(NULL)
}

```

```{R}
## Example numbers from slide deck - imaginary WBS PhD Salaries sample was 170K, SD was 46617.4
POPULATION.MEAN <- 165
POPULATION.SD <- 40

## Repeat at different sample size including 42 from example in class
junk <- lapply(c(5,10,42,100), function(N){
    ci.examp(mean.sim   = POPULATION.MEAN,
             sd         = POPULATION.SD,
             n          = N,
             reps       = 100,
             conf.level = 0.95,
             method     = "z",
             XLIM       = POPULATION.MEAN + 2 * POPULATION.SD * c(-1, 1)
             )
})

```

So, you can see that the confidence intervals for each of the 100 samples at a given sample size all cluster around the mean, but are wider as sample size is lower. That makes sense intuitively.

Looking specifically at our example N of 42, we can estimate which one of the 100 that might be, and see how the CI for sample means of about that size does include the real population mean of 165. You can also see there are a number of coloured CIs which do **not** include the sample mean. This shows how to interpret a CI.

Specifically, **your actual sample** is one of a very large amount of possible samples from the population (here we took 100 but we could have taken many more). The CI calculation essentially creates a CI with the width so that, out of 100 random samples, approximately 95 will actually contain the *true population mean*.

As such, we can see how it is clearly a measure of the **precision** of the estimate. For a compelling visualization of this, look how wide the CIs for N=5 are.

One thing to remember though is the 'law of large numbers' here. You might notice that not all of the CI plots contain exactly 5 intervals that do not contain the mean. This is analagous to the idea of coin flipping - it's just probability over very large numbers of trials, not saying that 'out of every 100, exactly 5 will not include the population mean'. But, over a very large number of trials, we can expect the amount of intervals that do not include the mean to tend towards 5%, in the same way that over a large number of coin flips, the proportion of heads tends towards 50%.

<!--chapter:end:07-statistics.Rmd-->

# Classical Statistical Hypothesis Testing {#H-testing}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)
library(skimr)
library(rstatix)
library(pwr)
```

This Chapter relates to Lecture 10, and contains some (re)worked examples from earlier sessions, focusing specifically on the hypothesis testing aspect of them.

## Correlation Significance Tests

Here, we revisit our correlation between GDP per capita, and Happiness metrics, which I pulled from Our Word in Data:

```{r}
Happy<-read_excel("Data/HappyGDP.xlsx", sheet = "2020")

summary(Happy)
head(Happy)
```

Let's not worry about plotting the data, and go straight to the correlation:

```{r}
Assoc1 <- cor.test(Happy$Happiness, Happy$GDPpc, 
                    method = "pearson")

Assoc1

```

Here are our results. The estimate is a correlation, and we test that using the **t statistic**. The *t-value* is simply the estimate divided by the standard error (which we can't see in this output), and is interpreted essentially as 'how far from 0 is the estimate, in standard errors'.

The *p*-value for t is very very small, and obviously less than 0.05.

Conclusion - reject null hypothesis, accept alternative hypothesis (as always, pending better evidence).

Importantly, this does not mean that the **true correlation in the population** is 0.745, simply that it is *very unlikely to be zero*.

We can then look at our estimate of 0.745, and - even better - our confidence interval (see Section \@ref(conf) for information on how to interpret confidence intervals), to gain some indication of the likely true correlation in the population.

## Regression Significance Tests

The process to asses the significance of regression estimates is very very similar to that for correlations. Let's revisit the heart disease data set we used earlier.

```{r}
Heart<-read_excel("Data/heart.data.xlsx")

summary(Heart)
head(Heart)
```

Let's go straight the the multiple regression model.

```{r}
heart.disease.lm<-lm(heart.disease ~ biking + smoking, data = Heart)

summary(heart.disease.lm)
```

We interpret these just as we did the correlation significance tests.

The t-value is large, and the *p*-value (two-tailed) is small.

Interestingly, here we are given 'stars' for the different levels of significance, so to some extent the software is doing some decision making for you. To be honest, I always caution against relying solely on looking for 'stars' (it's actually a bit of a running joke that I once told an entire class in the 1990s to 'just look at the stars'). That's because the actual significance or not decision is based on the critical value and one- or two-tailed decision. The software often makes an assumption of 0.05 critical value for *p*, two-tailed, and calculates the 'stars' based on that. Sometimes that can conflict with the decision you have made yourself about what should be significant or not. That can trip you up if you didn't know to change these values in the software package.

Further, it also sort of entrenches the idea that things can be 'more' or 'less' statistically significant. Take a look at the results above, you'll see 3 stars represents a significance of '0', and 2 stars represents '0.001', boring old '0.05' only gets a single star, and '0.1' gets a dot. I'm not a fan here because this encourages the analyst to post-hoc make decisions about 'marginally' significant, or 'very' significant. These concepts do not exist. You decide your critical value, and you either pass or fail it.

Am I perfect? No. Specifically, do any of my papers use the language of 'marginal' significance? Sure, I bet you could find it. I am never a fan though, and I can promise you I argued about it at the time!

<!--chapter:end:08-H-Testing.Rmd-->

# ANOVA {#ANOVA}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)
library(skimr)
library(rstatix)
library(pwr)


```

This Chapter provides analysis and examples related to the first part of Lecture 11.

Here, I'll demonstrate the basic application of ANOVA on the simple 3-group case of the Ed Sheeran Study, and go from there to introduce some classic 'concerns' with the statistical hypothesis testing methodology.

```{r}
ED<-read_excel("Data/SHEERAN_ANOVA.xlsx", sheet = "ANOVA")
ED$GROUP <- factor(ED$GROUP)
summary(ED)
head(ED)
```

Let's create a quick table of the group means, and visualize it with a simple boxplot of the groups.

```{r}
ED %>%
  group_by(GROUP) %>%
  get_summary_stats(ANGER, type = "mean_sd")
ggboxplot(ED, x = "GROUP", y = "ANGER")
```

Interesting picture. Group 1 is the control group, Group 2 is those who heard Ed Sheeran, Group 3 is those who heard other music. This is what we might call 'model-free' analysis. We are just looking at the data to see if intuitively it looks like we expect it to.

Let's run an ANOVA now, which places a statistical model on it, and tests whether or not the data supports the model.

```{r}
res.aov <- ED %>% anova_test(ANGER ~ GROUP)
res.aov
```

Results here suggest there is a significant effect (p-value is very small). We also have an *effect size* measure ('ges', or generalized eta-squared) which is very useful to us, and suggests the effect is quite large. This can be interpreted similarly to a regression coefficient (which is also an effect size measure), and is the amount of variance in the dependent variable (Anger) that is explained by group membership.

However, ANOVA only tests the *general effect* of the treatment / group. We don't know whether this is because of the difference between **all of the groups**, or only **some**. E.g., is it that there is an effect of music in general (i.e. between Control and Ed, and Control and Music, but not between Ed and Music), or that Ed specifically is anger-inducing (in which case we would see an effect between Ed and Music, and Ed and Control, and not between Music and Control).

## Post-Hoc Testing

We can investigate this using **post-hoc tests**, which compare the difference between each pair of groups. There are many different types of post-hoc test for ANOVA, but the most typical one to use is the Tukey's test, which is what we will do here.

```{r}
pwc <- ED %>% tukey_hsd(ANGER ~ GROUP)
pwc
```

We can actually plot these results in a really effective way:

```{r}
# Visualization: box plots with p-values
pwc <- pwc %>% add_xy_position(x = "GROUP")
ggboxplot(ED, x = "GROUP", y = "ANGER") +
  stat_pvalue_manual(pwc, hide.ns = TRUE) +
  labs(
    subtitle = get_test_label(res.aov, detailed = TRUE),
    caption = get_pwc_label(pwc)
    )
```

This very clearly tells us that it is the *Ed Sheeran group (2)* that is driving these results, and there isn't much to choose between the control group, and the 'music' group.

However, this type of post-hoc analysis has the potential for what is known as a *multiple comparisons problem*, which we might need to deal with.

Let's take a look **back in the slide deck...**

Remember, there are many other things that if we were doing ANOVA that we would also look to deal with - such as the various assumptions required of ANOVA, and so forth. But, they are beyond our scope in this class. Suffice to say that this has only scratched the surface of ANOVA so far.


<!--chapter:end:09-ANOVA.Rmd-->

# Issues with Significance Testing {#Issues}

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r}
#note: some of these aren't used for the code that I run, but for other parts I have hashed out.
library(tidyverse)
library(combinat)
library(gtools)
library(readxl)
library(ggplot2)
library(ggpubr)
library(mosaic)

library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(installr)
library(skimr)
library(rstatix)
library(pwr)

library(waffle)
```

This chapter provides examples and analysis for the second part of Lecture 11.

In this chapter, I'll explore examples of two common 'issues' or 'debates' regarding statistical significance testing, applied to two of our prior examples.

But first, let's unpack more about the Ed Sheeran Study, and the multiple comparisons problem.

## More on the Multiple Comparisons Problem

In this section, I'll provide some visualizations which can help us to understand the problem with multiple comparisons.

However, it's worth noting that the results depend on a number of assumptions about the research and what the real world actually looks like. The latter for sure we can never know. So, do bear that in mind.

First, let's visualize the situation I just discussed in the slides. We test 100 hypotheses, and in every case, the null is true in the population (remember, we cannot know the true state of the population in reality). Below, each grey box represents a hypothesis we test.

```{r}
x <- c(Hypotheses=100)
waffle::waffle(x, colors=c("lightgrey"))
#waffle::waffle(c(40, 60), colors=c("blue", "lightgrey")) + guides(fill=FALSE)
```

Now, remember, **even if all null hypotheses are true**, our significance level, or *p* = 0.05, so we have to expect that *on average* we would expect **5** of these hypotheses tests to return *p*-values \< 0.05, and thus we would reject the null. This is known as a **false positive**.

You might wonder how we could reduce the chances of false positive to zero. You could do this by setting your required *p* to be exactly zero. Do you see the problem here? You would certainly solve the false positive problem. But, you would also accept *all* null hypotheses. You would never actually find a statistically significant result in your research. Unless the null hypothesis really *was* true in the population in every single case, you would be inevitably missing out on making some discoveries.

Moving on, let's visualize the situation in hand here. Below, the same 100 hypothesis tests (remember, the null is true in all cases), but this time I have marked in red the expected level of false positives.

```{r}
x <- c(False_Pos.=5, True_Neg.=95)
#waffle::waffle(x, colors=c("lightgrey"))
waffle(x, colors=c("red", "lightgrey")) #+ guides(fill=FALSE)
```

Now, that is just the *expected* level for every 100 tests. If you remember from the discussion in Section \@ref(conf), it may not be you get *exactly* 5 false positives in 100 tests, just that you would expect the long run average number of false positives to be 5%, in an all-null world.

So, we can look at it through a different lens. What is the probability that you would get **at least one false positive** in any given batch of multiple tests.

That is known as the *familywise error rate*, and it applies whenever you can define a cluster or 'family' of tests.

It's the same principle as rolling a 6-sided die, and needing to score a '6'. Think of the '6' as the 'false positive' here.

In a single die role, the probability of getting a '6' is $\frac{1}{6}$, and of course the equivalent probability of *not* getting a '6' is $1-\frac{1}{6}=\frac{5}{6}$.

*But*, what if you get to roll the die 3 times? It's fairly easy to intuit that, even though the probability of getting a '6' on any single roll stays the same, the probability of getting a single '6' in 3 rolls is higher. In fact, we can calculate it.

To do so, we use the probability of *not* getting a 6, as follows:

*P*(no 6)=$(\frac{5}{6})^{3} = 0.579$

Recalling the laws of probability then...

*P*(at least one 6) = 1 - *P*(no 6) = 1 - .579 = .421

So, *now* we have a 42% probability of at least one 6 in 3 rolls, even though the probability of getting a 6 in any single roll is unchanged.

We can calculate the familywise error rate for any set of multiple comparisons in exactly the same way.

One of the key advantages of ANOVA is that it is a single test, and thus does not fall prey to the multiple comparisons problem. However, in the case of *post-hoc* testing for the Ed Sheeran study, we are doing 3 pairwise comparisons (i.e. 3 tests) across the 3 groups, with a stated significance level of 0.05.

For 1 test, the false positive rate is $1 - 0.95 = .05$

But, for 3 tests, the false positive rate is $1-0.95^{3}=0.14$

We are 'rolling the dice' multiple times.

In fact, below is a nice little calculator that demonstrates how the familywise error rate changes, based on the number of tests, and the chosen significance level. It was created by Dr Daniel Roelfs, during his time at the Norwegian Centre for Mental Disorders Research (NORMENT) in Oslo, and he kindly allowed me to use it here. Check out his website: <https://danielroelfs.com/about/>

What you could do is adjust the sliders to check the above calculations - what is the familywise error rate for 3 comparisons and a 0.05 significance level?

```{r}
knitr::include_app("https://danielroelfs.shinyapps.io/FWER_simple/",
  height = "700px")
```

```{r}
#<iframe src="https://danielroelfs.shinyapps.io/FWER_simple" width="100%" height="400px" #frameBorder="0"></iframe>
```

When you adjust the slides, it feels kind of scary right? There are many ways to deal with the problem, but they pretty much all amount to making some correction to the required significance level, making it more stringent, in order to reduce the false positives.

For example, in the Ed Sheeran post-hoc tests above, we used Tukey's test, which uses a correction for multiple comparisons. Another method dealt with below is the *Bonferoni Correction*, which is generally the most well-known of them.

## The Bonferroni Correction: Rate of Change in Football Goals per Season

Here, I'm using data from <https://www.footballhistory.org/league/premier-league-statistics.html>

I hand-entered this into a spreadsheet, and calculated the additional numbers myself.

```{r}
EPLGOALS<-read_excel("Data/EPLGOALS.xlsx")
head(EPLGOALS)
```

You can see here I have calculated the standard errors from the yearly goal totals (which represent that year's underlying rate of goal occurrence), then used that to calculate the 95% confidence Interval limits

We can use these to create a nifty chart with error bars, drawing from the code used by Spiegelhalter for Figure 9.4 in his book, available on his github:

<https://github.com/dspiegel29/ArtofStatistics/blob/master/09-4-homicide-rates-E%2BW/09-4-homicide-trends-x.Rmd>

```{r}
#modified from Spiegelhalter's Figure 9.4 available at:
#https://github.com/dspiegel29/ArtofStatistics/blob/master/09-4-homicide-rates-E%2BW/09-4-homicide-trends-x.Rmd

#note, the hashed-out code is not relevant to my example
#but left in in case someone else wants to use it

df<-EPLGOALS # read data to dataframe df
p <- ggplot(df, aes(x=Season, y=Goals)) # initial plot
p <- p + geom_bar(stat="identity", fill="red") +theme(axis.text.x = element_text(angle = 90))+scale_x_discrete(name="Season") # assign bar chart type, and x axis labels, rotated
#P <- p + theme(axis.text.x = element_text(angle = 90)) #rotate lables x axis

#p +geom_bar(stat="identity")+theme(axis.text.x = element_text(angle = #90))+scale_x_discrete(name="Country")+scale_y_continuous(name = "Survival Rate")

#yearLabels <- c("Apr  97-\nMar  98","Apr  00-\nMar  01","Apr  03-\nMar  #04","Apr  06-\nMar  07","Apr 09-\nMar  10","Apr  12-\nMar  13","Apr  #15-\nMar  16") # assign labels for x-axis

p <- p + geom_errorbar(aes(ymin=Lower95CI, ymax=Upper95CI), width=.1) # 95% intervals

#p <- p + scale_x_continuous(breaks=seq(1997, 2015, 3), labels =yearLabels) # attach labels and their break points

p <- p + scale_y_continuous(breaks=seq(0, 1100, 100)) # define break points for y-axis
p <- p + labs(y="Total Goals") # add y-axis label and caption
p

```

From this chart, and looking at the data itself, we can see that the 95% Intervals overlap, so it is hard to conclude that the underlying rate of goals has changed significantly year on year. Yes, even in the pandemic, despite what many football 'experts' said.

We can look at this in some more depth though. First, it's worth knowing that the UK Office for National Statistics uses this basic technique to estimate the probability of homicides, which also seem to be usefully approximated by the Poisson distribution.

Interestingly, the ONS suggest that it is over-stringent to rely on error bar overlap, and that we can also use z-tests to directly test the assumption that the change is zero.

See: <https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/compendium/focusonviolentcrimeandsexualoffences/yearendingmarch2016/homicide#statistical-interpretation-of-trends-in-homicides>

So using the z-scores in the data file, I have calculated the p-value (2 tailed as we do not hypothesize a direction for the difference) for the z-scores for the difference between each season, year-on-year.

```{r}
EPLP<-read_excel("Data/EPLGOALSP.xlsx")
summary(EPLP)
head(EPLP)
```

Here, we can plot the p-values (2-tailed), and again we see that two seasons seem to have significant differences. In other words, the p-values are less than 0.05 for the test as to whether the number of goals scored differs from the season before

```{r}
#visualize the z values simply with control lines
U <- 0.05
#L <- -1.96
p <- ggplot(EPLP, aes(x=Season, y=p2)) + geom_point() +theme(axis.text.x = element_text(angle = 90))+scale_x_discrete(name="Country")
p <- p+ geom_hline(aes(yintercept=U))
#p <- p+ geom_hline(aes(yintercept=L))
p

```

We can see that the 1999-2000 season, and the 2009-10 seasons have p values less than 0.05

The question is **are we suffering from the multiple comparisons problem**? Should we correct for it?

It's hard to say actually. Of course, we are indeed running multiple tests, 26 in fact. So, the chance of a false positive is high. The Bonferroni correction would immediately reduce the false positive chances, but at what cost?

Let's see how this would work. In order to calculate a Bonferroni correction, you can either adjust the p-value directly that you calculate for each test, or instead simply adjust the 'cutoff' value for p, known as the *critical p*, to lower it from 0.05 and make the significance test 'harder' to pass. The formula to adjust the cutoff value is simply:

$\alpha_b = (\frac{\alpha}{n})$

Where

$\alpha_b$ = Bonferroni-adjusted critical p value

$\alpha$ = original critical p value (here this is 0.05)

*n* = number of comparisons (here this is 26)

So, the formula gives us a new Bonferroni-adjusted critical p value of 0.0019

Let's see what happens with this new critical p value:

```{r}
#visualize the z values simply with control lines
U <- 0.0019
#L <- -1.96
p <- ggplot(EPLP, aes(x=Season, y=p2)) + geom_point() +theme(axis.text.x = element_text(angle = 90))+scale_x_discrete(name="Country")
p <- p+ geom_hline(aes(yintercept=U))
#p <- p+ geom_hline(aes(yintercept=L))
p

```

We can see that none of our tests now rejects the null. The Bonferroni correction is known as a highly conservative test. That is, it is based on the idea that the *null hypothesis is true* in each case in the population. We can thus consider it as the most stringent and conservative way to correct for the chance of false positives when doing multiple comparisons.

But, as I suggested above, that might not always be the best idea. Indeed, if you want to totally avoid any chance of a false positive, why not simply make the required alpha 0? Then, you would never get a false positive. Of course, you would never detect a **true positive** either.

What if it is the alternative hypothesis (that is, the H of an effect existing) that is true in all cases? In such cases, there can of course be *no false positives*. Therefore, in such a situation you would be *increasing the chances of a false negative* by reducing the chances of a false positive. So, what are the potential costs of each of these mistakes?

For example, Thomas Perneger's 1998 paper in the BMJ is scathing about the Bonferroni adjustment. Take a look at <https://www.bmj.com/content/316/7139/1236.full>

Mind you, I am not saying that's the final word, just that there are multiple perspectives on the issues!

It's never as simple as it seems when making statistical decisions, is it?

## Statistical Power: Brief Demonstration

Different types of analysis and research design require different types of power calculation, so it is hard to give a uniform example. But, for simplicity's sake, let's calculate the required sample size for the Ed Sheeran study we conducted earlier.

Remember, really, we should have done this **before collecting data**.

To calculate power, all we need the parameters of the experiment and analysis design. As such, it is easily possible to do this before collecting data, and to design your studies around it. Really, we should do this a lot more in business and management - it's routine in fields like medicine.

So, we had 3 groups, and used ANOVA

Let's set a significance of 0.05, a required power of 0.8, and assume the effect size is moderate (say 0.25)

```{r}
pwr.anova.test(k=3,f=.25,sig.level=.05,power=.8)
```

So, we really wanted to have around 50 in each group to have an 80% chance of detecting a moderate effect presuming the null was true.

You can see that my study (with only 15 in each group) was rather underpowered. However, if I had increased the effect size in the calculation to 0.5 (close to what the experiment suggested) this would have given me a result for n closer to what I actually used. However, you'd have to be VERY confident in the size of your likely effect to actually do that I think.

<!--chapter:end:10-Issues_w_sig.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`
When creating this book, I have used data from numerous publicly-available sources. I have also sometimes used code from publicly-available repositories and websites.

All sources are cited at the place they are used at this point, but in the future will be compiled here

Further, a number of open-access academic papers and other reports are cited or referred to in Chapters.

Weblinks are provided for all sources mentioned above where possible. However, any direct queries can be emailed to me, in case I have missed something, or perhaps a URL no longer works (all checked as of January 2024.)

<!--chapter:end:11-references.Rmd-->

