[["index.html", "WBS MRes: Learning From Data Chapter 1 Introduction and Orientation", " WBS MRes: Learning From Data Professor Nick Lee 2024-09-27 Chapter 1 Introduction and Orientation This is an online course book, designed for students on the Warwick Business School MRes Programme, who are taking the module ‘Learning from Data (and) Science’. The module consists of a set of in-person lectures, linked with these examples and additional expositions. You can see to the left a list of the contents of the book. However, please note that these ‘chapter numbers’ do not refer to the specific lectures. Not all of the lectures use quantitative examples. It is important to note that this module is not designed to teach you how to use R. The objective of the module is to help you learn how to use and interpret the results of quantitative social science research, understand the core principles of excellent quantitative social science, and also how to apply key principles of quantitative and scientific thinking into your own life and (where relevant) work. This book is written in Markdown, using R and the bookdown package, and published online in HTML format. A full archive of all the R Code and data for the individual chapters from the book is available from Professor Lee on request, once the book is completed for the year. This book is essentially a ‘live’ document, meaning that it is finalised for each year just before the module runs, but then modified for future runnings. The current year is 2024, and the book is currently in progress for 2024. Please enjoy the book, and learn from it. Feel free to email me with any questions or comments: nick.lee@wbs.ac.uk "],["intro.html", "Chapter 2 Describing the World With Data 2.1 Summarizing Data with Numbers 2.2 Box Plots 2.3 Violin Plots and Ridge Plots 2.4 Transformations", " Chapter 2 Describing the World With Data This Chapter is designed to work with Lecture 2. Here, I will demonstrate a number of important concepts about data displays and descriptions. First, let’s find some data to play with. In the following examples I will use some data from the Johns Hopkins Coronavirus Resource Center as at April 26 2022, which contains data by country about cases, deaths, and mortality for COVID. https://coronavirus.jhu.edu/ However, as of 2023, the JHCRC ceased collecting data, and their archival data is available on their github: https://github.com/CSSEGISandData/COVID-19 However, I do think it would be quite hard to recreate the exact snapshot in time of the data set we are using here. I’m going to use the top 20 countries in terms of raw number of cases To demonstrate how we can easily change people’s views of a data set, let’s run some bar charts: One thing we could do is change the orientation of the bars… That’s nice. The basic conclusion we can draw is everyone is pretty much the same in terms of survival rate, although Mexico is a bit down. Interestingly, it seems to me that the variation is more pronounced in this presentation where you look ‘down’ a line of results, than above where you look across. I’m not aware of any theory behind why, or even if anyone has ever noted that before, but it’s definitely something I noticed here. Either way, if I wanted to make everyone think that COVID survival rates were really different, there’s a simple trick I could use: Wow, how do you like that! What’s different? Changing the scale of the y-axis is an old trick, and when used to manipulate opinion it’s a bad thing. However, there is a case to say we need to know what we want to say, before we decide how to say it. When does ‘making sure we see the right message’ move into ‘outright manipulation / misrepresentation?’. Here’s another common way this type of data is presented in media and non-scientific reports: or a variant: Technically, it’s wrong to present this data in a line chart, because it is discrete values and does not represent a trend, but it is surprisingly common - probably because it ‘looks more sciency’… Ironically, there is some logic behind using line charts with this sort of data, depending on what you are trying to get across - let’s move to excel to demonstrate… While we’re at it, we’ll also look at pie charts… Quick transition to MS Excel, then back to the slides… 2.1 Summarizing Data with Numbers Above, we simply plotted a single variable of interest, and showed that some very basic changes to how it was displayed had the potential to ‘lead’ the reader to different conclusions. Even that simple tweaking is a powerful way to change opinion, and used in the wrong way can be very misleading. Now, let’s think about the idea of an even simpler way to describe data - by summarizing it with just some numbers. For this example, I am going to use data on GDP per head in 2020 for each nation, from the World Bank. Before I do anything else, I’m going to graph it again. This time I will use a histogram, which is similar to a bar chart, but not the same thing. Specifically, a bar chart is used when we are displaying discrete categories. So, above, we had 20 countries, and a single bar for each country. A histogram is instead when we divide our data into ‘groups’ of similar values, which are called ‘bins’. Thus, each ‘bar’ on a histogram represents the number of elements (in this case, countries) which fall in that bin. So, in this case, we can divide the GDP variable into bins. As an example, we can divide GDP per head into ‘bins’ of US$10000, and plot a histogram As you can see, there are nearly 120 countries in the first bin, which you can also see runs from -5000 to 5000 dollars. Of course, we know there are no countries with negative GDP per head, but this is just an artefact of the binning process. This sort of chart gives us something very different to a bar chart. Instead, it is really a frequency distribution, and it charts the frequency of values of a given variable - in this case GDP per head. So, here we have some idea of the distribution of the variable. It’s clearly skewed quite heavily, with some outliers at the high end. Let’s move on to summarize this variable with some numbers: ## ## GDP2020 ## --------------- ----------- ## Mean 15162.61 ## Std.Dev 22146.53 ## Min 233.84 ## Median 6117.49 ## Max 173688.19 ## N.Valid 251.00 ## Pct.Valid 94.36 What we have here are the classic ‘summary statistics’ for 2020 GDP per head. One thing missing - which admittedly is not very useful in this case, is the Mode, or the ‘most frequent’ value. ## [1] 1875.441 There you have it, the mode is 1875.441. One important thing to note here is that the word average is badly misused when talking about data. Specifically, the mean, median, and mode can all at one time or the other be referred to as the average value of a given variable. But, as you can see, they are all different numbers! The word ‘average’ is a very vague word to use in this context, so do be careful about when you use it. You should always default to using the specific term you are talking about. A few other summary statistics are calculated for 2020 GDPph below ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 233.8 2127.5 6117.5 15162.6 18072.4 173688.2 15 ## [1] 15944.94 ## [1] 173454.4 What we have here now includes the range (173454.4) and the interquartile range (15944.94), which are both useful measures of how ‘spread out’ the data is. Each of the summary numbers above has strengths and weaknesses when describing different types of data. Here, we will mainly talk about the sensistivity to outliers. Specifically, the median and interquartile range are less sensitive to outliers as measures of central tendency and range respectively than their counterparts the mean and the range. So, that’s why you commonly see the median used so frequently in these sorts of situations. But, what do we mean by outliers? Let’s look at this in the context of another graphical display, the box plot. 2.2 Box Plots The box plot (and the other displays below) are criminally underused in my opinion. Most of us calculate a box plot at school, and then never again. But, it’s a really useful way to understand a data set. Let’s do a boxplot for the 2020 GDPph data: A box plot presents a lot of information very efficiently. The median is the thick horizontal, the white rectangle is bounded by the interquartile range, and outliers are represented as dots. We can see here that the data is very skewed, and there are some very significant outliers at the high end (no prizes for guessing which countries they are). A weakness of a box plot is that it doesn’t actually show the distribution of the data. One way to do this is to add what is called jitter, which actually plots the values of each case, with a ‘jitter’ to avoid overlapping identical values. So, we can see here that most of the data is clustered towards the bottom of the range - i.e. very low GDPph. This is of course the same conclusion as the historgram allowed us to draw. But, the box plot also gives us plenty of other information about the variable. Even so, while box plots are nice, they can also obscure important facts about the distribution. Adding the jitter helps, but there are other ways to display this type of data that make the distribution more obvious. 2.3 Violin Plots and Ridge Plots Here, I’m going to give you a set of even more underused data displays, which can show some powerful things about variables. In order to do so, I am going to simulate some data, to make sure it has exactly the features I want to show. First, let’s create the data and use box plots to display it, such as you are already experienced with above. That’s all very well, but as we know, this can obsure the distribution. So, let’s add jitter: Well, look at that. B has what is called a bimodal distribution. Lets look at what is called a violin plot to emphasize this: Neat! You can see how the violin plot really emphasizes the differences in the distributions, which are totally obscured by the box plots, and only partially shown by adding the jitter. Another way to visualize this is with ridges, which basically compare the distributions of variables together: These are really cool in my view. And, if you are asking me, my preference in this type of situation would be to combine jittered box plots, which show the basics, and emphasize the outliers and summary numbers, with ridge plots, which show a really nice visualization of the distributions of each variable. Taken together, these plots allow us to understand a surprisingly large amount of important information about variables. OK, let’s move back to the slides to discuss another crucially important concept related to data distributions: transformations. Back to slide deck 2.4 Transformations As you can see, the data is heavily skewed, or ‘squished’ towards the bottom. This makes it hard to see any patterns by eye. We can make it easier to interpret by transforming the variable. Here, using a logarithmic transform. Let’s run the box again: Nice, see how the spread is easier to interpret here? Log transforms are essentially ‘inverse exponentials’, so what they do is bring down extreme high outliers. Let’s see this in some more plots: Compare the violin of the log transformed variable with the original: In fact, we can use the log transform to revisit some of our COVID data back in EXCEL… Quick transition to MS Excel, then back to slides… "],["norm_dist.html", "Chapter 3 The Normal Distribution 3.1 A Simple Demonstration of the Normal Distribution 3.2 What Actually is a Normal Distribution?", " Chapter 3 The Normal Distribution This chapter supplements the content in Lecture 3. 3.1 A Simple Demonstration of the Normal Distribution First, let’s consider the ‘empirical distribution’, which is the pattern your data set makes. Let’s consider UK births in 2020 for our example, since they’re a subject close to my heart (my first child was born in 2020). Looking online, one can pull down the distribution of UK birthweights in 2020, available from the UK ONS website, and which are reported in 500g ‘bins’. Source: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/birthsummarytablesenglandandwales/2020#births-data We can use these numbers to create a chart (please note, the code I use for the demonstrations in this lesson is mostly modified from David Spiegelhalter’s originals, available on his Github page, with some of my own bits and bobs and new data…) Spiegelhalter’s Github can be found here: https://github.com/dspiegel29/ArtofStatistics The first thing I want to do is create a data set that accurately reflects the ONS distribution of UK birthweights in 2020. To do that, I use the following code, which in this instance I will display below, with annotations. weights=c(1500, 2000, 2500, 3000, 3500, 4000, 4500,5000,5500) #modified #categories from 2020 UK Data mids=weights-250 # Note the DS original is +250, but this does not work for UK #data as the numbered is the UPPER bound of the weight category in the ONS stats. #So the babies in the &#39;2000&#39; bin weigh 1500-1999, so the midpoint is 1750, not #2250 n=c(5015, 7103, 27554, 99220, 218442, 179088, 53530,6952,635) # numbers in each #bin, UK2020 N=sum(n) # total number of babies area=N*500 # number * binwidth = total area of histogram #I think lbw should be sum of groups 1-3 not 1-2 as in the book code. Using 1-3 #gives a result for the UK of 6.6% which tallies with the official stat given #that I remove the &#39;no recorded weight&#39; and &#39;implausible&#39; categories&#39;. I imagine #that the DS book data has categories which may be the lower bound not the upper #as in the UK data table. lbw = sum(n[1:3]) # number with low birth weight (less than 2500) lbw.percent=100*lbw/N # % low birth weight # 6.6% which tallies with Nuffield stat for 2020. Remember though, the above just sets up the numbers required to recreate the empirical distribution, it isn’t analysis itself. Next, I want to summarize the distribution I just created and a few important numbers regarding Low Birthweight that we will need to use for our later discussion. Displayed below are the total N for the distribution, the number of low birthweight (LBW) babies born, the percentage of LBW babies, the mean, the standard deviation, the skewness, and kurtosis. ## [1] 597539 ## [1] 39672 ## [1] 6.639232 ## [1] 3345.263 ## [1] 577.7594 ## [1] 0 ## [1] 1.77 Next, I will calculate the estimated proportion of low birthweight babies using a normal distribution approximation, and see if it tallies OK with the figure that is taken from the distribution of 6.6% ## [1] 7.173338 I will also calculate the middle birthweight proportion as a sense check - should be very close to 50%! ## [1] 49.98182 For our reference, we will calculate the 25th, 50th, 75th percentiles ## [1] 2955.57 ## [1] 3345.263 ## [1] 3734.956 Next I want to calculate the birthweight percentile of Monty, my son. ## [1] 0.8366843 Now, at last, we can do some plots! First let’s set up the histogram and overlay a normal distribution on to it. The weight of Monty is overlaid as a red line. We can estimate if this is ‘unusual’ or not by how far away it is from the mean of the distribution. 3.2 What Actually is a Normal Distribution? Below, I will chart the normal distribution with Standard Deviations. Mathematically, around 95% of the area under the chart is between -2 and +2 SDs, and 99.8% between -3 and +3 SDs Now, another way to think about this is that the area under the normal curve represents 100% of the probability of a given value (here, weight) occuring. So, you could say that it is around 95% probable that any given birthweight (randomly sampled) will occur between -2 and +2 SDs of the mean. We can calculate this easily, given that the mean is 3345.263, and the SD is 577.7594 So, it is 95% probable that a random baby born in the UK in 2020 will weigh between 2189.745 and 4500.781g We can calculate how many SDs Monty’s weight was above the mean by calculating the z-score for his weight of 3912g. To do so, let’s return to the slide deck and see the calculation. Move back to the slides… Next, we can overlay percentiles in 5% increments on the chart, which are also useful ways to describe a distribution like the Normal. 50th is of course the median, and the 25th and 75th percentilies are the quartiles. Here, the same things we have previously discussed in relation to samples are applied to populations We can also see that Monty lies on the 83rd percentile, that is 83% of babies born in 2020 weighed less. This tallies with what we were told at the time, and then things are monitored quite closely from that time on in terms of percentiles. Moving to a discussion of low birthweight. It is medically defined as low if below 2500g. Nuffield reports that 6.6% of babies in 2020 were born at low birthweight, which is fairly good. This tallies fairly well with the predicted proportion from the normal distribution of 7.2% What we gain from the grey shaded area is the proportion of the population which is expected to be born at low birthweight, but also an indication of the probability that a randomly-chosen baby born in 2020 would be of low weight. This concept can easily be transferred to other weights in this population (and by extension to other variables and populations distributed like this) "],["assoc_rel.html", "Chapter 4 Associations and Relationships 4.1 Correlations and Associations 4.2 Introducing Nonlinear Associations 4.3 Regression 4.4 Multiple Regression 4.5 Visualizing Multiple Regression", " Chapter 4 Associations and Relationships This chapter supplements the content in Lecture 5. 4.1 Correlations and Associations First, let’s look at the basic concepts of correlation, using some data from the World Happiness Report, and GDP from the World Bank, all put together by the team at Our World in Data. You can explore their work here: https://ourworldindata.org/ First, I can just double check the data by describing it. ## vars n mean sd median trimmed mad ## Country* 1 249 125.00 72.02 125.00 125.00 91.92 ## Happiness 2 153 5.49 1.12 5.53 5.52 1.16 ## GDPpc 3 197 20463.88 20717.34 12655.00 17037.01 13338.95 ## Pop 4 242 59178643.60 331869505.09 5596196.00 12318073.38 8185922.38 ## min max range skew kurtosis se ## Country* 1.0 2.490000e+02 2.480000e+02 0.00 -1.21 4.56 ## Happiness 2.4 7.820000e+00 5.420000e+00 -0.26 -0.38 0.09 ## GDPpc 731.0 1.125570e+05 1.118260e+05 1.58 2.55 1476.05 ## Pop 809.0 4.663087e+09 4.663086e+09 11.65 152.44 21333379.77 We’ve got 249 countries, with population, GDP per capita, and Happiness. There’s a fair bit of missing data, which you can see by looking at the numbers in the n column. That’s ok for now, as long as we are aware of it. Let’s plot this data using a very basic scatterplot. Interpretation: there’s seemingly an association here. As one variable increases, the other does too. It’s worth noting that I have chosen to put GDPpc on the x-axis, which implies that the driver of Happiness is GDP. This replicates the way that most of the media articles that are regularly written on this kind of topic do it. Nevertheless, this is just a scatterplot, so there is no real requirement to put any particular one of the variables on the x-axis, because it’s important that you do NOT draw any causal conclusion from this very basic scatterplot. Simple associations are not in any way conclusive evidence of any causality in the relationship, since there are many other things which could be going on that are impossible to tease out simply with a bivariate association seen on a scatterplot. For example, there could be a spurious relationship - what could cause both happiness and GDPpc to increase? Ideas? Could it simply be something like ‘political stability’? Or something else? Either way, all we have here is a basic data display. While, as I showed previously, simply displaying data can be very powerful, it is still only essentially an ‘eyeball’ analysis, and different people might draw different conclusions here (we’ll discuss that later on). What we want to do, is have some sort of objective metric for the relationship, and for this, we have the correlation statistic. ## ## Pearson&#39;s product-moment correlation ## ## data: Happy$Happiness and Happy$GDPpc ## t = 13.502, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6636288 0.8092331 ## sample estimates: ## cor ## 0.745184 Here, we can see that the correlation is 0.75 But what does that mean? We can return to the slides for some explanation. 4.2 Introducing Nonlinear Associations Let’s return to the scatterplot of GDPpc and Happiness As the Economist analysis discussed in the lectures suggests, this association also looks kind of nonlinear. However, unlike the Economist, we can also say that there are multiple ways to look at this: First, you could take the Economist’s view that this is probably a nonlinear relationship. Indeed, we can check this out by transforming GDPpc and comparing the results between transformed and non-transformed data. Let’s have a try at that. What we will do is transform the GDPpc variable using a log transform (i.e. multiplying GDPpc by the natural logarithm). ## vars n mean sd median trimmed mad ## Country* 1 249 125.00 72.02 125.00 125.00 91.92 ## Happiness 2 153 5.49 1.12 5.53 5.52 1.16 ## GDPpc 3 197 20463.88 20717.34 12655.00 17037.01 13338.95 ## Pop 4 242 59178643.60 331869505.09 5596196.00 12318073.38 8185922.38 ## GDPpc_log 5 197 9.38 1.14 9.45 9.41 1.33 ## min max range skew kurtosis se ## Country* 1.00 2.490000e+02 2.480000e+02 0.00 -1.21 4.56 ## Happiness 2.40 7.820000e+00 5.420000e+00 -0.26 -0.38 0.09 ## GDPpc 731.00 1.125570e+05 1.118260e+05 1.58 2.55 1476.05 ## Pop 809.00 4.663087e+09 4.663086e+09 11.65 152.44 21333379.77 ## GDPpc_log 6.59 1.163000e+01 5.040000e+00 -0.25 -0.79 0.08 We can see there is a new variable now, GDPpc_log, which is the log transformed GDPpc variable - basically the original GDPpc variable multiplied by the natural logarithm. Of course, there are many other possible transforms. I touched on them earlier, in Chapter 2, and I’ll elaborate a little more on it below, for now let’s rerun the plot using GDPpc_log That relationship definitely looks more linear, and if that was my only criteria for deciding what to do with the data, I would definitely now run with this idea, as the Economist did in their article With that in mind, let’s take a quick look at the correlations for these variables. First the original GDPpc, and then the log transformed one. ## ## Pearson&#39;s product-moment correlation ## ## data: Happy$Happiness and Happy$GDPpc ## t = 13.502, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6636288 0.8092331 ## sample estimates: ## cor ## 0.745184 ## ## Pearson&#39;s product-moment correlation ## ## data: Happy$Happiness and Happy$GDPpc_log ## t = 15.916, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7287785 0.8487456 ## sample estimates: ## cor ## 0.7964703 We can see that the association is stronger (the correlation is higher) for the log GDPpc variable, although to be honest, it is not a very big difference here, compared to other data sets that I have seen discussed in the media. That’s interesting in itself (note we are using 2020 data, so maybe the pandemic has something to do with it?) However, transformations of data are kind of a complicated topic though. In essence, what we are doing is changing the distribution of one (or more) of the variables, in a systematic way, in order to convert a non-linear relationship (which may be unable to be tested with methods like correlation and regression) into a linear one. There are many different types of transformations available to you, and the one you choose depends on the form of the nonlinearity. A log transform converts an exponential (or similar) relation into a linear one. But, there are specific transforms for many different forms. Oftentimes, it’s not always obvious what exactly to do, and people often default to transforming any nonlinear-looking relationship, or highly skewed variable, using log or maybe the quadratic (squaring). It’s not really that simple though, and I think we need to be a bit more logical and theory-driven on why we transform variables. In this case, I admit that the logic makes sense, that there is still a relationship between GDPpc and happiness at the top end, but you just have to increase GDPpc by a lot more to get the same change in happiness, compared to the change needed at the lower end of GDPpc. And, making that transform to come to that conclusion actually makes a difference to how we think of the influence of GDPpc on happiness, and could concievably lead to policy changes compared to the prior non-transformed analysis. Even so, I have always had a slightly different possibility on my mind. In fact, it may be that there are two groups of country, low and high income, and different linear associations within those groups. That’s a similar-ish idea to the nonlinear one, but it is a bit different, and I actually prefer that idea. A nice challenge would be to work out a way to test which of the two alternative explanations is actually better supported by the data… Maybe I’ll leave that for another day though. We can return to the slides for a bit more discussion. 4.3 Regression We can add a regression line to the GDPpc / Happiness scatterplot, for some extra information over the correlation. We can also do this with the log GDPpc to compare. You can see that using the log GDPpc variable does improve things, but not by that much in my view. That said, it’s clear that the line is a bit more ‘precise’ (the errors are lower which can be seen by the narrower shaded regions). So, if we are looking for the ‘best’ model, it’s pretty clear that the log GDPpc model is better than the simple GDPpc model. Either way, the regression line adds a layer of information on to the correlation, which allows us to predict y from x. How so? Well, let’s add the regression equation to the chart to see: This adds an intercept, and with that plus the coefficient we have all that we need to plot a straight line, which we can extrapolate to higher values of GDPpc (although remember this is the logged GDP variable) and predict what the happiness scores would be. This is obviously viable to the extent we can justify the relationship, and also within the parameters of our variables, and how confident we are that the relationship is consistent at all levels of the variables. return to the slides 4.4 Multiple Regression Here, we will use a simple three-variable set of simulated data, which represents rates of smoking, rates of cycling, and heart disease incidence. This data is available from: https://www.scribbr.com/statistics/linear-regression-in-r/ Remember, this is simulated data not real data. ## vars n mean sd median trimmed mad min max range ## ...1 1 498 249.50 143.90 249.50 249.50 184.58 1.00 498.00 497.00 ## biking 2 498 37.79 21.48 35.82 37.71 27.51 1.12 74.91 73.79 ## smoking 3 498 15.44 8.29 15.81 15.47 10.86 0.53 29.95 29.42 ## heart.disease 4 498 10.17 4.57 10.39 10.18 5.42 0.55 20.45 19.90 ## skew kurtosis se ## ...1 0.00 -1.21 6.45 ## biking 0.07 -1.22 0.96 ## smoking -0.04 -1.12 0.37 ## heart.disease -0.03 -0.93 0.20 Next, let’s run three simple regressions among the three variables Remember, these are simple regression lines, with bivariate scatterplots. What I mean, is the effect of biking on heart disease does not take account of the effect of smoking on heart disease. What we need to do is run a model which takes account of both the predictors, as we can explain on the slide-deck… Now we can run the Multiple Regression Model ## ## Call: ## lm(formula = heart.disease ~ biking + smoking, data = Heart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1789 -0.4463 0.0362 0.4422 1.9331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.984658 0.080137 186.99 &lt;2e-16 *** ## biking -0.200133 0.001366 -146.53 &lt;2e-16 *** ## smoking 0.178334 0.003539 50.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.654 on 495 degrees of freedom ## Multiple R-squared: 0.9796, Adjusted R-squared: 0.9795 ## F-statistic: 1.19e+04 on 2 and 495 DF, p-value: &lt; 2.2e-16 First, we look at the R2, or more accurately the Adjusted R2, which takes account of the number of variables (they are usually very similar). The R2 essentially measures how much variation in your Dependent Variable is explained by your model (i.e. your regression predictors). It ranges from 0 to 1, and higher means more variance explained. Here we see a very high AdjR2 of 0.98, almost 1. This is simulated data of course, and it is quite rare to see such high R2s in real data. For the most part, you want R2 to be high, but if it gets too high (say above 0.8 but this is not a ‘rule’) you might start to get worried about whether your IVs and DVs are actually distinct things. Apart from that, standards of what makes an acceptable R2 are highly dependent on the context and field you are working in, and you’ll no doubt see lots of different ideas throughout your career on what is acceptable. Of course, that only scratches the surface of this issue, and I’ll say more in the lecture slides. Next, we look at the coefficients, and they can be interpreted as follows: A 1 unit increase in biking will on average lead to a 0.2 unit decrease in heart disease A 1 unit increase in smoking will on average lead to a 0.17 unit increase in heart disease. Of course, what these ‘units’ refer to depends on what you have measured them with of course. But, the unarguable interpretation is that the effect is strong. However, because the scales of the variables are different, its not really possible to compare the sizes of the effects. So to some extent we don’t know which of the two variables has the ‘bigger’ effect here, relatively at least. To do that, we need to standardize the coeffiecients, which basically means putting all the variables on the same scale. Beforehand, they all were on different scales, which you can see by going back up and looking at the range, mean, and standard deviation of the original variables. You can see for yourself they are all different. We can solve this issue by creating a new set of standardized data, and running the model on that, as follows: ## vars n mean sd median trimmed mad min max range skew ## ...1 1 498 0 1 0.00 0 1.28 -1.73 1.73 3.45 0.00 ## biking 2 498 0 1 -0.09 0 1.28 -1.71 1.73 3.43 0.07 ## smoking 3 498 0 1 0.05 0 1.31 -1.80 1.75 3.55 -0.04 ## heart.disease 4 498 0 1 0.05 0 1.19 -2.10 2.25 4.35 -0.03 ## kurtosis se ## ...1 -1.21 0.04 ## biking -1.22 0.04 ## smoking -1.12 0.04 ## heart.disease -0.93 0.04 You know these variables are standardized as they now all have a mean of 0 and a standard deviation of 1. Now, let’s run our model on this standardized data set. ## ## Call: ## lm(formula = heart.disease ~ biking + smoking, data = std_Heart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.47658 -0.09762 0.00792 0.09671 0.42283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.119e-17 6.410e-03 0.00 1 ## biking -9.403e-01 6.418e-03 -146.53 &lt;2e-16 *** ## smoking 3.234e-01 6.418e-03 50.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1431 on 495 degrees of freedom ## Multiple R-squared: 0.9796, Adjusted R-squared: 0.9795 ## F-statistic: 1.19e+04 on 2 and 495 DF, p-value: &lt; 2.2e-16 You can see the things that have changed are the ‘Estimates’, which are now standardized. In decimal, they are -0.94 for biking and 0.32 for Smoking. The correct interpretation of these standardized effects is that for every 1SD increase in biking, you expect a 0.94SD decrease in heart disease, and for every 1SD increase in smoking, you would expect a 0.32 increase in heart disease. So, we can see that actually biking has a much higher relative effect on heart disease than smoking (although in the opposite direction). That said, this assumes that both the two IVs have similar standard deviations, and distributions, and we didn’t check that in this example. 4.5 Visualizing Multiple Regression There are loads of different ways to visualize multiple regression. It’s not trivial, because we have more than two variables, so we can’t use the techniques we used already. Some people like to use 3D-style plots, which look cool, but are not always easy to interpret, and take quite a lot of extra work, for what I would say is not that much payoff (if any). In this case, we can use a pretty simple visualization, where we could plot the relationship of biking to heart disease at different levels of smoking. This would be a fairly typical way to do things if we thought the relationship between biking and heart disease changed according to the level of smoking, in which case it would be a moderator. Here, it doesn’t really work that way, but it’s a cool visualization regardless. It does require some data prep, but not that much, and I took the basic idea from the website where I sourced the data: https://www.scribbr.com/statistics/linear-regression-in-r/ Here, we can see that the effect of smoking is really just to raise the likelihood of heart disease, however much biking you do. So, for a given person who bikes a given amount, if they smoke more they will have a higher risk than a person who bikes the same amount but smokes less. But, for a given smoker, the more they bike, the lower their risk of heart disease, to the extent that if a heavy smoker bikes enough, their actual risk of heart disease could even be lower than a non-smoker who does not bike at all. Don’t forget, this is not real data, but the point stands. Get on your bike. "],["uncertainty.html", "Chapter 5 Beginning to Understand Uncertainty 5.1 Demonstration: Sampling from a ‘Known’ Population", " Chapter 5 Beginning to Understand Uncertainty This Chapter relates to lecture 6. In this chapter I’ll introduce core concepts around uncertainty in our results. Understanding that the results of our analysis always contain some level of uncertainty is probably the most critical concept to get our heads around as quantitative social scientists. Most of our job is not really coming up with the actual statistics, such as the correlation coefficient, or regression beta, but is more about understanding how to interpret and use those results - i.e. what they mean. And, fundamental to that is understanding their uncertainty. Again, to reiterate the message I sent in class, many of the examples in this Chapter, and later ones, involve randomness. This means that the results here may be slightly different numerically to the results in the slides. And, if you were to run these examples yourself, you would also get slightly different results. This is nothing to worry about, because the meaning of the results does not change. So, to start the journey, let’s grab some data. Here, we will again use the simple three-variable set of simulated data, which represents rates of smoking, rates of cycling, and heart disease incidence. ## # A tibble: 6 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.8 10.9 11.8 ## 2 2 65.1 2.22 2.85 ## 3 3 1.96 17.6 17.2 ## 4 4 44.8 2.80 6.82 ## 5 5 69.4 16.0 4.06 ## 6 6 54.4 29.3 9.55 Rather than do the full ‘describe’ as I did in the last chapter, I have simply above looked at what is called the ‘head’ of the data set or the first few rows. This is because all I want to do here is double check that I have the data, and what variables are there. Let’s calculate some simple summary statistics from this data set to build on. For example, what is the mean and median for ‘smoking’? ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.5259 8.2798 15.8146 15.4350 22.5689 29.9467 Now, we know this is really simulated data, but let’s imagine for now that it was actually obtained by an organization like the Office for National Statistics in the UK, using a survey. We can presume the study was done well, and thus it is based on a true random sampling method, and we assume that the study population matches whatever target population we have in mind (remember the ‘inference gaps’ discussed in class). What we really want to know is, how close are these statistics (i.e. the mean and median) to the true population values that we would have found if we could survey the entire target population? Let’s begin to think about this by starting to build a table using these statistics, by going back to the slide deck… 5.1 Demonstration: Sampling from a ‘Known’ Population Now, let’s go back one more step, and demonstrate the uncertainty inherent to sample statistics by way of example. Let’s now assume that this sample of 498 people actually is the population we are interested in. What this means is, we can actually draw a sample from this population of 498 and see what happens. First, let’s present the distribution for the entire ‘population’ of 498. Now, let’s literally take a sample of 10 random cases from that population of 498. Here, we are sampling without replacement, and are thus essentially doing exactly what a hypothetical ‘researcher’ would do if they drew a random sample of 10 people to complete their survey, from the population of 498. ## # A tibble: 10 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 89 28.1 14.7 12.8 ## 2 123 48.3 12.0 7.30 ## 3 494 47.7 27.6 11.3 ## 4 256 73.1 11.3 1.69 ## 5 64 19.9 6.08 11.6 ## 6 255 51.9 27.3 9.59 ## 7 213 15.5 18.8 14.9 ## 8 321 3.96 27.5 19.4 ## 9 356 66.0 3.51 2.72 ## 10 178 31.2 5.57 9.86 Next, let’s look at the relevant statistics (median and then mean) and distribution of this sample of 10: ## [1] 13.37157 ## [1] 15.43435 We can do the same for successively larger samples, say 50, and 200: ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 382 37.2 22.4 10.5 ## 2 389 21.5 4.76 10.4 ## 3 426 15.4 23.4 14.1 ## 4 114 2.62 4.32 14.9 ## 5 349 68.0 13.3 4.07 ## 6 11 51.8 14.4 6.43 ## 7 476 25.9 8.16 10.9 ## 8 440 30.5 18.1 11.7 ## 9 128 49.3 5.09 6.07 ## 10 180 50.2 4.13 5.54 ## # ℹ 40 more rows ## [1] 16.33627 ## [1] 16.01781 ## # A tibble: 200 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 249 45.7 23.8 9.23 ## 2 75 4.49 23.4 18.8 ## 3 438 31.8 22.2 13.3 ## 4 251 35.7 8.92 10.5 ## 5 3 1.96 17.6 17.2 ## 6 329 15.1 3.28 12.7 ## 7 11 51.8 14.4 6.43 ## 8 441 74.2 9.30 1.57 ## 9 45 22.6 13.7 13.5 ## 10 5 69.4 16.0 4.06 ## # ℹ 190 more rows ## [1] 15.32976 ## [1] 15.32976 As you can see, the distributions of the smaller samples are more peaky and bumpy, because they are very sensitive to individual data points. As the sample gets larger, it starts to look more like the population right? We can complete our table now in the slides of the sample statistics (median and mean) showing that in general, as we get closer to the population size, the statistics generally get closer too. To do so, let’s go back to the slides… "],["bootstrap.html", "Chapter 6 Introduction to Bootstrapping 6.1 Bootstrapping in the Context of Previous Examples 6.2 Bootstrapping Other Stuff…", " Chapter 6 Introduction to Bootstrapping This Chapter supplements the material in the first part of Lecture 7. In this chapter, we will learn the core concepts of bootstrapping. That is, creating synthetic sampling distributions through multiple resampling (with replacement) of a single sample. The basic process is fairly simple, once you have your original sample, and has the following characteristics: A bootstrap sample has an equal probability of randomly drawing any of the original sample elements (data points). Each element can be selected more than once - because the sample is done with replacement. Each resampled data set (the new sample) is the same size as the original one. First, I will demonstrate the basic principle. Recall from the last chapter that there was a simulated data set of 498 people, with variables representing smoking, biking, and heart disease. We treated this as the population and then sampled from it to demonstrate uncertainty at different sample sizes. So, let’s take the sample of 50 that we drew from that ‘population’ of 498, and imagine that we got this sample by (for example) doing a survey of the population (of 498), and this is our data set for analysis. First we will remind ourselves of the properties and distribution of our sample, the median, mean, and distribution: ## [1] 16.33627 ## [1] 16.01781 Ok, there we go. Remember, this sample is our ‘data set’ for analysis. We know the median and mean, but we have no real indication of the uncertainty in those estimates. In order to get that information, we will eventually use the bootstrap method. For now, we are just going to demonstrate the basic idea. So, what we do now is draw another random sample of 50 from this 50, but each time we draw a data point, we replace it back, so we are always drawing our sample from the full 50. This is called sampling with replacement. In this way, the new sample can only contain values which were in the original sample, but can contain different frequencies of those values. Or in other words, each value can occur many different times, and that number of times may be different to the original sample. So the distribution of values in this new sample will be different to the original sample, and the statistics will therefore also be different. Let’s draw this new sample, take the median and mean of the sample, and plot it: ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 290 63.8 29.1 7.62 ## 2 40 45.8 4.72 7.04 ## 3 443 22.5 16.3 13.7 ## 4 334 13.9 27.3 16.0 ## 5 286 25.7 23.3 13.6 ## 6 67 23.3 14.3 13.3 ## 7 121 65.6 14.1 4.04 ## 8 162 22.3 23.2 15.9 ## 9 273 24.7 4.60 12.1 ## 10 113 69.5 23.2 5.75 ## # ℹ 40 more rows ## [1] 14.70505 ## [1] 14.5281 Marvelous! Now, for the purpose of example, let’s draw two more of these resamples from the original 50, take their median and mean, and plot the distributions… ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 423 71.9 19.4 4.11 ## 2 443 22.5 16.3 13.7 ## 3 113 69.5 23.2 5.75 ## 4 108 22.8 9.82 10.9 ## 5 192 42.4 11.0 8.83 ## 6 355 70.0 20.3 3.69 ## 7 283 67.8 26.9 6.26 ## 8 444 2.01 1.75 14.0 ## 9 156 54.4 16.3 7.59 ## 10 334 13.9 27.3 16.0 ## # ℹ 40 more rows ## [1] 16.33627 ## [1] 15.61665 ## # A tibble: 50 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 426 15.4 23.4 14.1 ## 2 268 34.2 17.8 11.3 ## 3 418 62.0 23.9 6.96 ## 4 389 21.5 4.76 10.4 ## 5 322 61.3 15.4 5.48 ## 6 447 33.2 17.6 12.1 ## 7 30 25.1 5.84 11.5 ## 8 423 71.9 19.4 4.11 ## 9 345 50.9 29.8 10.8 ## 10 444 2.01 1.75 14.0 ## # ℹ 40 more rows ## [1] 17.5565 ## [1] 17.07838 Now, if we return to the slides, we can build a table using these mean and median values. Of course, the slide deck will have slightly different values, since it’s based on a different run of the resampling process, but the principle is the same. So, this is the basic idea of bootstrapping. We sample with replacement from our original sample, many many times. We did 3 here manually, but we generally use a program to do this many more times, such as a thousand or more. 6.1 Bootstrapping in the Context of Previous Examples To further reinforce the point, let’s now place ourselves in the position of three different researchers, each of varying levels of enthusiasm, and all three are researching the same population of 498 people that we have already explored in the last few examples. Researcher 1 is a little like me as a Ph.D. student, and maybe more interested in ‘experiencing life’. So, he has little time to actually collect data, and not much more enthusiasm for it. In the end, he manages to take a sample of 10 people from the population of 498. Researcher 2 is a bit more enthusiastic, and gets a sample of 50. Researcher 3 is fairly conscientious, and takes a sample of 200 from the population of 498. Now, what we can do, is run 1000 bootstrap replications of each of these varying-sized subsamples of the population, to see what might happen: First, the 10: ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sub.10, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 15.43435 -0.01622465 2.979889 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% ( 9.61, 21.29 ) ## Calculations and Intervals on Original Scale Now let’s do it for the other two subsamples of n=50, and n=200 ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sub.50, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 16.01781 -0.005925671 1.082005 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (13.90, 18.14 ) ## Calculations and Intervals on Original Scale ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = sub.200, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 15.3508 0.007735695 0.5796773 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (14.21, 16.48 ) ## Calculations and Intervals on Original Scale We’ll now build a table with these values back in the slide deck. IAgain, remember the values in the slide deck will differ from these due to the randomness of the process. Now, let’s shift our minds a bit, and consider that the data set of 498 actually represents a sample of a larger population (remember from the last chapter, it’s simulated, but meant to represent a sample from the population). So, let’s bring in Researcher 4, the most conscientious of all. She is the one who manages to take a sample of 498 people from the population. And, finally, we can bootstrap the original full sample of 498: ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Heart, statistic = f1, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 15.43503 0.001437866 0.359051 ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = results, type = &quot;norm&quot;) ## ## Intervals : ## Level Normal ## 95% (14.73, 16.14 ) ## Calculations and Intervals on Original Scale This is a very nice set of results, which can tell us many interesting things. So let’s go back to the slides….. 6.2 Bootstrapping Other Stuff… We have so far only bootstrapped the mean. However, the basic principle can be applied to virtually any statistical estimate. So, we can revisit some of our prior analyses, and use the bootstrap method to quantify the uncertainty in the estimates that we previously accepted without really thinking too hard about them. 6.2.1 Correlations First, let’s revisit our recent correlation analysis of Happiness and GDP per capita. ## # A tibble: 6 × 4 ## Country Happiness GDPpc Pop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2.4 1971 38972236 ## 2 Albania 5.2 13192 2866850 ## 3 Algeria 5.12 10735 43451668 ## 4 American Samoa NA NA 46216 ## 5 Andorra NA NA 77723 ## 6 Angola NA 6110 33428490 ## vars n mean sd median trimmed mad ## Country* 1 249 125.00 72.02 125.00 125.00 91.92 ## Happiness 2 153 5.49 1.12 5.53 5.52 1.16 ## GDPpc 3 197 20463.88 20717.34 12655.00 17037.01 13338.95 ## Pop 4 242 59178643.60 331869505.09 5596196.00 12318073.38 8185922.38 ## min max range skew kurtosis se ## Country* 1.0 2.490000e+02 2.480000e+02 0.00 -1.21 4.56 ## Happiness 2.4 7.820000e+00 5.420000e+00 -0.26 -0.38 0.09 ## GDPpc 731.0 1.125570e+05 1.118260e+05 1.58 2.55 1476.05 ## Pop 809.0 4.663087e+09 4.663086e+09 11.65 152.44 21333379.77 If we run the same analysis as in Chapter 2, we’ll get the same results: Correlation R = 0.75 Now, let’s take uncertainty into account, by bootstrapping that correlation and creating some confidence intervals. ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.689 0.811 ## Response: Happiness (numeric) ## Explanatory: GDPpc (numeric) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 0.745 So, you can see the correlation is 0.75 with a 95% confidence interval of 0.69 - 0.81 Now, let’s extend this to the multiple regression case we have previously used, examining the relationships between smoking, biking, and heart disease. ## # A tibble: 6 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.8 10.9 11.8 ## 2 2 65.1 2.22 2.85 ## 3 3 1.96 17.6 17.2 ## 4 4 44.8 2.80 6.82 ## 5 5 69.4 16.0 4.06 ## 6 6 54.4 29.3 9.55 ## vars n mean sd median trimmed mad min max range ## ...1 1 498 249.50 143.90 249.50 249.50 184.58 1.00 498.00 497.00 ## biking 2 498 37.79 21.48 35.82 37.71 27.51 1.12 74.91 73.79 ## smoking 3 498 15.44 8.29 15.81 15.47 10.86 0.53 29.95 29.42 ## heart.disease 4 498 10.17 4.57 10.39 10.18 5.42 0.55 20.45 19.90 ## skew kurtosis se ## ...1 0.00 -1.21 6.45 ## biking 0.07 -1.22 0.96 ## smoking -0.04 -1.12 0.37 ## heart.disease -0.03 -0.93 0.20 Here, we need to calculate multiple confidence intervals as we have multiple estimates. ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 intercept 15.0 ## 2 smoking 0.178 ## 3 biking -0.200 ## # A tibble: 3 × 3 ## term lower_ci upper_ci ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 biking -0.203 -0.197 ## 2 intercept 14.8 15.1 ## 3 smoking 0.171 0.186 It’s worth reflecting on exactly what these conflidence intervals mean, and to do so, we can move back to the slides… "],["t-test.html", "Chapter 7 T-Tests for Means 7.1 Independent Sample T-Tests 7.2 Paired-Sample T-tests", " Chapter 7 T-Tests for Means This Chapter supplements the material in the second part of Lecture 7. In this chapter, I will introduce the t-test, a classic analysis technique that is used to compare two groups together. 7.1 Independent Sample T-Tests We can also use bootstrapping as an entry point to a new analysis situation, where we are comparing two groups. This could be for example in a classic experimental context; treatment and control. Remember, t-tests can be done in any analysis setting, and it does not require bootstrapping at all. It just so happens that they are nicely explainable at this point. So, we are going to analyze a set of data from my infamous Ed Sheeran Study1 - which would certainly win an Ignobel Prize if I were ever to do it in reality rather than in my fondest imaginings. The design was discussed in the lecture slides, but in brief we have a two-group ‘treatment and control’ experiment, the classic ‘randomized control trial’ style study. The first thing to do is read in the data and describe it. ## # A tibble: 6 × 3 ## ID GROUP ANGER ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 4 ## 2 2 2 5 ## 3 3 1 2 ## 4 4 2 3 ## 5 5 2 4 ## 6 6 1 2 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## ID 1 30 15.50 8.80 15.5 15.50 11.12 1 30 29 0.00 -1.32 1.61 ## GROUP 2 30 1.50 0.51 1.5 1.50 0.74 1 2 1 0.00 -2.07 0.09 ## ANGER 3 30 3.33 1.21 3.0 3.33 1.48 1 5 4 0.04 -1.30 0.22 Table 7.1: Data summary Name ED_IND Number of rows 30 Number of columns 3 _______________________ Column type frequency: factor 1 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts GROUP 0 1 FALSE 2 1: 15, 2: 15 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1 15.50 8.80 1 8.25 15.5 22.75 30 ▇▇▇▇▇ ANGER 0 1 3.33 1.21 1 2.00 3.0 4.00 5 ▁▇▇▆▇ We can see that we have one FACTOR variable, which we need to indicate the group each subject was in (CONTROL being GROUP 1, and ED being GROUP 2). So, let’s run an independent samples T-Test with bootstrapped confidence interval. We use an independent samples test, as the theory is these two groups - after participating in the experiment - are no longer the ‘same’. There is something ‘different’ about the population of people who listen to Ed Sheeran, compared to those who do not. What we are trying to do is assess whether this difference is in anger. ## # A tibble: 2 × 2 ## GROUP name ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 2.8 ## 2 2 3.87 ## Response: ANGER (numeric) ## Explanatory: GROUP (factor) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 -2.65 ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -5.49 -0.575 Remember: Group 1 is the control, and Group 2 listened to Ed Sheeran. Cool, so it seems that Group 2 displayed more anger. The confidence interval for the t-statistic does not contain 0, so it supports the idea that there is a difference here. The interval is quite wide though - because of our small sample size. In a later Chapter we’ll return to this issue of what confidence intervals actually mean. 7.2 Paired-Sample T-tests OK, so let’s use a different design, using a paired samples t-test. Let me go back to the slides… to explain this difference. The same basic process is needed, but with some modifications because of the type of comparison we are doing. And, as such, we have some new data. ## ID ANG_T1 ANG_T2 ## 1 1 1 4 ## 2 2 2 5 ## 3 3 3 2 ## 4 4 4 3 ## 5 5 2 4 ## 6 6 1 2 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## ID 1 30 15.50 8.80 15.5 15.50 11.12 1 30 29 0.00 -1.32 1.61 ## ANG_T1 2 30 2.00 1.02 2.0 1.88 1.48 1 4 3 0.76 -0.57 0.19 ## ANG_T2 3 30 3.33 1.21 3.0 3.33 1.48 1 5 4 0.04 -1.30 0.22 Now, for this bootstrap purpose we actually need to to calculate the difference between the two measurements (here, T1 and T2). Then, we bootstrap a one-sample t-test with this difference variable. This essentially tests whether the difference is different from zero. If it is, that indicates some effect of the treatment. Let us first calculate the new variable: ## ID ANG_T1 ANG_T2 DIF ## 1 1 1 4 3 ## 2 2 2 5 3 ## 3 3 3 2 -1 ## 4 4 4 3 -1 ## 5 5 2 4 2 ## 6 6 1 2 1 ## vars n mean sd median trimmed mad min max range skew kurtosis ## ID 1 30 15.50 8.80 15.5 15.50 11.12 1 30 29 0.00 -1.32 ## ANG_T1 2 30 2.00 1.02 2.0 1.88 1.48 1 4 3 0.76 -0.57 ## ANG_T2 3 30 3.33 1.21 3.0 3.33 1.48 1 5 4 0.04 -1.30 ## DIF 4 30 1.33 1.27 1.0 1.38 1.48 -1 4 5 -0.13 -0.62 ## se ## ID 1.61 ## ANG_T1 0.19 ## ANG_T2 0.22 ## DIF 0.23 We can see there is a new ‘DIF’ variable here. Next, we bootstrap a confidence interval for the mean of the difference variable, to see whether it includes zero: ## Response: DIF (numeric) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 1.33 ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9 1.73 ## [1] 2 ## [1] 3.333333 Marvelous. We can see that the results suggest that after listening to Ed Sheeran, our sample on average reported more anger. This is because the 95% confidence interval does not include 0, and therefore I am confident in saying that there is probably some effect going on here. Not surprised… 1If you are a friend or relative of, or more importantly a lawyer for, Ed Sheeran, please note that this is not a real study. "],["probability.html", "Chapter 8 Introduction to Probability 8.1 Simulating Coin Flips 8.2 Premier League Goals and the Poisson Distribution 8.3 Birth Weight and the Normal Distribution: Brief Example", " Chapter 8 Introduction to Probability This Chapter provides examples and additional content for Lesson 8. In this lesson, we will learn about the classical theory of probability, and how it can be used to construct measures of uncertainy of our estimates, and from there we can begin to draw inferences from the sample to the population. 8.1 Simulating Coin Flips The first thing we’ll do is simulate 5000 fair coin flips. Remember, this is another random process, so results will differ each time slightly ## Heads Trial Cum_Heads Pct_Heads ## 1 1 1 1 1.0000000 ## 2 0 2 1 0.5000000 ## 3 0 3 1 0.3333333 ## 4 1 4 2 0.5000000 ## 5 1 5 3 0.6000000 ## 6 0 6 3 0.5000000 Below, we’ll plot the results of this simulation for ease of interpretation The thing to notice (in combination with the table of results above the plot) is that eventually the proportion of heads versus tails approaches 50%, but it does not start out that way. In other words, for a fair coin toss sequence, the trials (tosses) are independent, which means that for any given toss of the coin, the chance of a head or tail is 50%. But, that’s not the same as saying that you’ll get equal numbers of heads or tails for every (or any) particular sequence of multiple tosses - it’s not ‘self-correcting’ in any other way than sheer weight of numbers. So, eventually over enough trials, the sequence should start to look like 50/50 heads and tails. But, that is not the tosses ‘self-correcting’, simply that you are tossing a lot of coins. This is at the root of a number of fallacies about predicting random events, such as the Gambler’s fallacy. 8.2 Premier League Goals and the Poisson Distribution Note, much of the inspiration and code for this section comes from: https://bookdown.org/theqdata/honors_thesis/goal-scoring-and-the-poisson-process.html The data comes from: https://www.football-data.co.uk/englandm.php and is the EPL results for 21-22 Season ## # A tibble: 6 × 107 ## Div Date Time HomeTeam AwayTeam FTHG FTAG ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 E0 2021-08-13 00:00:00 1899-12-31 20:00:00 Brentford Arsenal 2 0 ## 2 E0 2021-08-14 00:00:00 1899-12-31 12:30:00 Man United Leeds 5 1 ## 3 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Burnley Brighton 1 2 ## 4 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Chelsea Crystal … 3 0 ## 5 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Everton Southamp… 3 1 ## 6 E0 2021-08-14 00:00:00 1899-12-31 15:00:00 Leicester Wolves 1 0 ## # ℹ 100 more variables: TOTG &lt;dbl&gt;, FTR &lt;chr&gt;, HTHG &lt;dbl&gt;, HTAG &lt;dbl&gt;, ## # HTR &lt;chr&gt;, Referee &lt;chr&gt;, HS &lt;dbl&gt;, AS &lt;dbl&gt;, HST &lt;dbl&gt;, AST &lt;dbl&gt;, ## # HF &lt;dbl&gt;, AF &lt;dbl&gt;, HC &lt;dbl&gt;, AC &lt;dbl&gt;, HY &lt;dbl&gt;, AY &lt;dbl&gt;, HR &lt;dbl&gt;, ## # AR &lt;dbl&gt;, B365H &lt;dbl&gt;, B365D &lt;dbl&gt;, B365A &lt;dbl&gt;, BWH &lt;dbl&gt;, BWD &lt;dbl&gt;, ## # BWA &lt;dbl&gt;, IWH &lt;dbl&gt;, IWD &lt;dbl&gt;, IWA &lt;dbl&gt;, PSH &lt;dbl&gt;, PSD &lt;dbl&gt;, ## # PSA &lt;dbl&gt;, WHH &lt;dbl&gt;, WHD &lt;dbl&gt;, WHA &lt;dbl&gt;, VCH &lt;dbl&gt;, VCD &lt;dbl&gt;, ## # VCA &lt;dbl&gt;, MaxH &lt;dbl&gt;, MaxD &lt;dbl&gt;, MaxA &lt;dbl&gt;, AvgH &lt;dbl&gt;, AvgD &lt;dbl&gt;, … Next, let’s see a few important bits of descriptive data. First, let’s calculate the total number of games in a season: ## Length Class Mode ## 380 character character Next, how many goals in the season were there? ## [1] 1071 And, what was the mean number of goals per game? ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 3.000 2.818 4.000 9.000 OK, so now we have some idea of the goals per game, let’s plot the distribution of goals. If you’ve ever seen a Poisson distribution, you’ll recognise this! Let’s check this out. First, let’s create a table of all the matches with different numbers of goals. ## # A tibble: 9 × 2 ## TOTG ActualMatches ## &lt;dbl&gt; &lt;int&gt; ## 1 0 22 ## 2 1 65 ## 3 2 88 ## 4 3 81 ## 5 4 60 ## 6 5 43 ## 7 6 17 ## 8 7 3 ## 9 9 1 Let’s pull some stats of the Total Goals variable which well help us in a second. ## min Q1 median Q3 max mean sd n missing ## 0 2 3 4 9 2.818421 1.626359 380 0 Importantly, because the Poisson distribution is described only by its mean, the first and most basic check we can do is whether the mean and the variance of the variable are the same (or at least very close). We already have the mean above (2.82), but what is the variance? ## [1] 2.645042 This is not exactly the same, but good enough to be going on with. So, it seems to me at least that we can use the Poisson distribution to at least somewhat accurately describe the probability of occurance of a given number of total goals scored in a premier league game, for this season at least. The first thing I am going to do with that information is build a figure which compares the actual numbers of goals scored in games with the predicted numbers which would be scored if the goals scored were a perfect Poisson distribution. Ideally, we would build the Poisson probabilities for 0-9 goals (which is the maximum number scored in 21-22). However, if you do this, you will find that the two tables (actual and predicted goals) will have different numbers of rows. This is because there were no games with 8 goals scored in 21-22, but one with 9. This is a bit annoying, but not a big issue. What we do is build a Poisson probability distribution for 0-8 goals, and treat the final probability as that for ‘8 or more’ goals, for the purposes of drawing our figure. This isn’t strictly correct, because we have not collected up our actual goal data into that category, but we could do that if we wanted to. Later, we’ll break this down properly, but for our showy graph here we don’t really need to do it. ## PoisProb ## 1 0.059700132 ## 2 0.168260108 ## 3 0.237113915 ## 4 0.222762284 ## 5 0.156959477 ## 6 0.088475579 ## 7 0.041560239 ## 8 0.016733465 ## 9 0.005895244 So, this table of probabilities, based on the Poisson distribution, allows us to make some predictions of what we could expect. Remembering again that there are 380 games in a season, we can see that there is a 0.16% chance of seeing 4 goals in a game, which equates to 380 x 0.16 = 60.8 games we would expect to see in a season with 4 goals. We can check this out by creating a new table comparing actual with predicted values… ## # A tibble: 9 × 3 ## TOTG ActualMatches ExpectedMatches ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 22 23 ## 2 1 65 64 ## 3 2 88 90 ## 4 3 81 85 ## 5 4 60 60 ## 6 5 43 34 ## 7 6 17 16 ## 8 7 3 6 ## 9 9 1 2 Remember, as you can see above, the row for ‘8’ is missing, and it goes straight to 9. So, to repeat, just take (for simplicity’s sake here) the final row of Expected Matches as meaning ‘2 games with 8 or more goals’ Anyway, it’s pretty scarily close! E.g. in 21-22 there were 60 matches with 4 goals, and that is identical to the predicted amount (the difference with the predicted 60.8 from the probability distribution is likely a rounding error). Let’s plot this for the big payoff: Wow, that is pretty accurate! Remember, the ‘expected’ figures are purely drawn from the entirely theoretical Poisson distribution with a mean of 2.82. They very closely match the actual data. To be fair though, that graph is really just a bit of show, but what we want is the ‘go’. In other words, what could we do with this information? Well, there is a lot of money spent on sports betting. We could use this information to help us decide whether we should place a bet that there will be a game in the 22-23 season with 10 goals? - assuming that this is being written before the 22-23 season (which is true). Remember, there were 380 games in 21-22, none of which contained 10 goals. So, for ease of thinking about this, let’s make the assumption that the PL started in 2021, and there were no games before to count (in reality, we would go back to the last time 10 goals were scored and count from there, but let’s go with this in the first instance, and we’ll expand later on, promise). So, first, we need to calculate a new set of probabilities out to 10 goals. ## PoisProb ## 1 0.0597001317 ## 2 0.1682601080 ## 3 0.2371139154 ## 4 0.2227622837 ## 5 0.1569594775 ## 6 0.0884755792 ## 7 0.0415602392 ## 8 0.0167334647 ## 9 0.0058952437 ## 10 0.0018461421 ## 11 0.0005203206 ## 12 0.0001333166 So, the number we want is the second-last probability: 0.00052. This is because the table starts from the probability of 0 goals. This means we can expect a game with 10 goals to happen every 1923 games Calculate this by dividing 1 by the probability. So, given there are 380 games per season, you would expect a game with 10 goals to happen once every 5 seasons. So, if I started counting from the 21-22 season, the answer is NO, I would not bet on there being a game with 10 goals in the 22-23 Premier League season. But, let’s add some (pretty obvious) further context, as of November 2022, there have been 5 games in the history of the Premier League where 10 goals have been scored (and 1 with 11). The Premier League has been going since 1992, and so far in Nov 22 there have been…30 seasons. So, we are probably due one. AND, the last game with 10 goals was in 2013 (Man Utd 5, West Brom 5) So, I reckon we are definitely-maybe-probably due one. By the way, as of the start of the 22-23 season, there had been 21 matches with 9 goals. We would expect given our Poisson distribution that a game with 9 goals should happen every 1.4 seasons, meaning over 30 seasons we would expect…. 21. 8.2.1 2023 Update So, the above example is totally unchanged (other than some typo corrections and nicer wording) from when I originally coded it in 2022. The million dollar question: was there a game with 10 goals in the 22-23 season…? The answer is no, there wasn’t. But there were two with 9! Luckily I am not a betting man. I do think we are due one, so this season (2024) is probably a good bet, but as of January 20th 2024, we still haven’t had one… Unless of course you think something has fundamentally changed about the premier league in recent years, changing the probability of goals (and thus suggesting recent seasons are drawn from a Poisson distribution with different characteristics than past ones), which is in fact an interesting question in its own right… 8.3 Birth Weight and the Normal Distribution: Brief Example Let’s consider UK births in 2020, since they’re a subject close to my heart, as my son was born that year. Looking online, one can pull down the distribution of UK birthweights in 2020, available from the UK ONS website, and which are reported in 500g ‘bins’. Source: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/birthsummarytablesenglandandwales/2020#births-data We can use these numbers to create a chart, and see what it looks like, inspired by David Spiegelhalter’s originals, which are available on his Github page. Spiegelhalter’s Github can be found here: https://github.com/dspiegel29/ArtofStatistics ## [1] 3345.263 ## [1] 577.7594 The first number is the mean, the second is the standard deviation. What you can see here is, just using the real data from 2020 UK birthweights, it looks very much like what is called a ‘normal distribution’, or what you might have heard called a ‘bell curve’. We can almost certainly treat any given child’s birthweight as if it was a random variable drawn from a normal distribution, with a mean and standard deviation that is given by the empirical data here (3345.263 and 577.7594 respectively). "],["statistics.html", "Chapter 9 Introduction to Statistics 9.1 The Distribution of Sample Means, and the Central Limit Theorem 9.2 Demonstrating Confidence Intervals", " Chapter 9 Introduction to Statistics This Chapter builds on the material in Lesson 9. Here, we begin to apply our probability concepts to the task of statistical inference in the ‘classical’ way. The theory explained and demonstrated here is the foundation of the (very large) majority of the quantitative business and management research you will read and use. Almost any time someone refers to a finding being (statistically) significant or a p-value, they are using this idea. However, a major weakness in many social science fields - business and management being one - is that a rather large proportion of researchers don’t actually understand the core principles of statistical inference, and are instead often just copying what other people have written, or told them to do. So, if you can get your head around the next 3-4 chapters, you’ll be in a strong position to work with what’s already out there, and also do your own solid research. 9.1 The Distribution of Sample Means, and the Central Limit Theorem The first thing to cover is an extension of the ideas in the last chapter about sampling distributions. It’s called the Central Limit Theorem, and is often the source of much angst in statistics students (including myself). However, here I am actually going to demonstrate how it works, and show how is actually quite amazing. To do so, I will use a synthetic data set of monthly incomes from 20 people. Let us treat this data set as the population. ## # A tibble: 6 × 2 ## Person Income ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 5600 ## 2 B 6000 ## 3 C 6400 ## 4 D 6800 ## 5 E 7200 ## 6 F 7600 ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5600 7500 9400 9400 11300 13200 ## [1] 2366.432 Above, you’ll see the important descriptive statistics, including the standard deviation of 2366.432. Let’s plot the frequency distribution, and once we do so we will find that it is essentially uniform - every value occurs once and only once in the population. Now, what I am going to do is take a single random sample of 2 from that population. This sample is taken ‘without replacement’, if you are wondering, and so it is in essence exactly as if a researcher went into the field and took a sample from this population. ## [1] &quot;One Random Combination of 2 of the 20 Income Values&quot; ## [1] 11600 7200 Let’s take the mean of the sample: ## res1 ## 1 11600 ## 2 7200 ## [1] 9400 The mean of the random sample of 2 will of course be different each time the sample is taken (e.g. when writing this text, the mean was 10600, but this may not be the same in the version you are reading!) Just for reference, the mean of the population (remember, the 20 people) was 9400 So, let’s now do that for every possible combination of 2 values from this population. Given there are n=20 values in the population, there are k=190 possible combinations of two values. Note: This is not the same as bootstrapping - we are not sampling with replacement here. We are instead taking combinations. The thing to think about is that this is the equivalent of taking every single possible sample of 2 that you could take from this population. Remember the ping-pong balls! Let’s do it: ## [1] &quot;Every Possible Combination of the 20 Income Values&quot; ## [,1] [,2] ## [1,] 5600 6000 ## [2,] 5600 6400 ## [3,] 5600 6800 ## [4,] 5600 7200 ## [5,] 5600 7600 ## [6,] 5600 8000 ## [7,] 5600 8400 ## [8,] 5600 8800 ## [9,] 5600 9200 ## [10,] 5600 9600 ## [11,] 5600 10000 ## [12,] 5600 10400 ## [13,] 5600 10800 ## [14,] 5600 11200 ## [15,] 5600 11600 ## [16,] 5600 12000 ## [17,] 5600 12400 ## [18,] 5600 12800 ## [19,] 5600 13200 ## [20,] 6000 6400 ## [21,] 6000 6800 ## [22,] 6000 7200 ## [23,] 6000 7600 ## [24,] 6000 8000 ## [25,] 6000 8400 ## [26,] 6000 8800 ## [27,] 6000 9200 ## [28,] 6000 9600 ## [29,] 6000 10000 ## [30,] 6000 10400 ## [31,] 6000 10800 ## [32,] 6000 11200 ## [33,] 6000 11600 ## [34,] 6000 12000 ## [35,] 6000 12400 ## [36,] 6000 12800 ## [37,] 6000 13200 ## [38,] 6400 6800 ## [39,] 6400 7200 ## [40,] 6400 7600 ## [41,] 6400 8000 ## [42,] 6400 8400 ## [43,] 6400 8800 ## [44,] 6400 9200 ## [45,] 6400 9600 ## [46,] 6400 10000 ## [47,] 6400 10400 ## [48,] 6400 10800 ## [49,] 6400 11200 ## [50,] 6400 11600 ## [51,] 6400 12000 ## [52,] 6400 12400 ## [53,] 6400 12800 ## [54,] 6400 13200 ## [55,] 6800 7200 ## [56,] 6800 7600 ## [57,] 6800 8000 ## [58,] 6800 8400 ## [59,] 6800 8800 ## [60,] 6800 9200 ## [61,] 6800 9600 ## [62,] 6800 10000 ## [63,] 6800 10400 ## [64,] 6800 10800 ## [65,] 6800 11200 ## [66,] 6800 11600 ## [67,] 6800 12000 ## [68,] 6800 12400 ## [69,] 6800 12800 ## [70,] 6800 13200 ## [71,] 7200 7600 ## [72,] 7200 8000 ## [73,] 7200 8400 ## [74,] 7200 8800 ## [75,] 7200 9200 ## [76,] 7200 9600 ## [77,] 7200 10000 ## [78,] 7200 10400 ## [79,] 7200 10800 ## [80,] 7200 11200 ## [81,] 7200 11600 ## [82,] 7200 12000 ## [83,] 7200 12400 ## [84,] 7200 12800 ## [85,] 7200 13200 ## [86,] 7600 8000 ## [87,] 7600 8400 ## [88,] 7600 8800 ## [89,] 7600 9200 ## [90,] 7600 9600 ## [91,] 7600 10000 ## [92,] 7600 10400 ## [93,] 7600 10800 ## [94,] 7600 11200 ## [95,] 7600 11600 ## [96,] 7600 12000 ## [97,] 7600 12400 ## [98,] 7600 12800 ## [99,] 7600 13200 ## [100,] 8000 8400 ## [101,] 8000 8800 ## [102,] 8000 9200 ## [103,] 8000 9600 ## [104,] 8000 10000 ## [105,] 8000 10400 ## [106,] 8000 10800 ## [107,] 8000 11200 ## [108,] 8000 11600 ## [109,] 8000 12000 ## [110,] 8000 12400 ## [111,] 8000 12800 ## [112,] 8000 13200 ## [113,] 8400 8800 ## [114,] 8400 9200 ## [115,] 8400 9600 ## [116,] 8400 10000 ## [117,] 8400 10400 ## [118,] 8400 10800 ## [119,] 8400 11200 ## [120,] 8400 11600 ## [121,] 8400 12000 ## [122,] 8400 12400 ## [123,] 8400 12800 ## [124,] 8400 13200 ## [125,] 8800 9200 ## [126,] 8800 9600 ## [127,] 8800 10000 ## [128,] 8800 10400 ## [129,] 8800 10800 ## [130,] 8800 11200 ## [131,] 8800 11600 ## [132,] 8800 12000 ## [133,] 8800 12400 ## [134,] 8800 12800 ## [135,] 8800 13200 ## [136,] 9200 9600 ## [137,] 9200 10000 ## [138,] 9200 10400 ## [139,] 9200 10800 ## [140,] 9200 11200 ## [141,] 9200 11600 ## [142,] 9200 12000 ## [143,] 9200 12400 ## [144,] 9200 12800 ## [145,] 9200 13200 ## [146,] 9600 10000 ## [147,] 9600 10400 ## [148,] 9600 10800 ## [149,] 9600 11200 ## [150,] 9600 11600 ## [151,] 9600 12000 ## [152,] 9600 12400 ## [153,] 9600 12800 ## [154,] 9600 13200 ## [155,] 10000 10400 ## [156,] 10000 10800 ## [157,] 10000 11200 ## [158,] 10000 11600 ## [159,] 10000 12000 ## [160,] 10000 12400 ## [161,] 10000 12800 ## [162,] 10000 13200 ## [163,] 10400 10800 ## [164,] 10400 11200 ## [165,] 10400 11600 ## [166,] 10400 12000 ## [167,] 10400 12400 ## [168,] 10400 12800 ## [169,] 10400 13200 ## [170,] 10800 11200 ## [171,] 10800 11600 ## [172,] 10800 12000 ## [173,] 10800 12400 ## [174,] 10800 12800 ## [175,] 10800 13200 ## [176,] 11200 11600 ## [177,] 11200 12000 ## [178,] 11200 12400 ## [179,] 11200 12800 ## [180,] 11200 13200 ## [181,] 11600 12000 ## [182,] 11600 12400 ## [183,] 11600 12800 ## [184,] 11600 13200 ## [185,] 12000 12400 ## [186,] 12000 12800 ## [187,] 12000 13200 ## [188,] 12400 12800 ## [189,] 12400 13200 ## [190,] 12800 13200 ## [1] &quot;Number of combinations without repetition&quot; ## [1] 190 That was a rather long exhibit up there, but I wanted to show you that this was actually really done properly, and we have 190 combinations. Next, let us create the means of each of the 190 combinations ## X1 X2 MDIST ## 1 5600 6000 5800 ## 2 5600 6400 6000 ## 3 5600 6800 6200 ## 4 5600 7200 6400 ## 5 5600 7600 6600 ## 6 5600 8000 6800 That’s just the first 6 (the head of the data set), but you can rest assured that there are 190 means created here. OK, so here is the kicker. Let’s plot a histogram of these 190 means. Remember, a histogram is a frequency distribution: Well well well! Even though the original population was a completely uniform distribution with a mean of 9400, the distribution of all of the sample means looks quite a lot like a normal distribution / bell curve! Let’s overlay one on it as well to make the point… Further, let’s take the mean of those means… ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5800 8200 9400 9400 10600 13000 Hello! It turns out, the mean of those means is the population mean!!! This, in a demonstration, is the central limit theorem. Let’s move back to the slides to talk about this some more. 9.2 Demonstrating Confidence Intervals Below I will demonstrate the idea of confidence intervals as a measure of precision, and at the same time help to really lock in the correct interpretation of what a confidence interval really means. To do so, I will continue the in-class example of WBS Ph.D. graduate salaries. However, here, I will simulate the population distribution that we have sampled from. Let’s recap: Sample size (N) = 42 Sample mean = 170,000 Sample SD = 46617.4 I will simulate a population distribution that this sample could quite conceivably have come from. It will have the following features: Population mean = 165,000 Population SD = 40000 I will also assume that the population distribution is normal. That is, the distribution of all business and management Ph.D. starting salaries is shaped like a bell curve - the normal distribution (remember birth weights from the last chapter). This is a reasonable assumption in my view, given the features of the phenomena we are investigating. The important thing to remember is not really the population distribution of salaries (which is just used here to simulate the population), but the sampling distribution of the means. Remember, the central limit theorem shows us that whatever the shape of the population distribution we are sampling from, the sampling distribution of the means will take the form of a normal distribution, with a mean of the population mean. So, below, I will simulate the population, and then calculate 100 confidence intervals for 4 different sample sizes (5, 10, 42, 100), to demonstrate two things: How confidence intervals get more precise as sample size increases, and How to interpret your specific confidence interval as simply one possibility of many. So, you can see that the confidence intervals for each of the 100 samples at a given sample size all cluster around the mean, but are wider as sample size is lower. That makes sense intuitively. Looking specifically at our example N of 42, we can estimate which one of the 100 that might be, and see how the CI for sample means of about that size does include the real population mean of 165. You can also see there are a number of coloured CIs which do not include the sample mean. This shows how to interpret a CI. Specifically, your actual sample is one of a very large amount of possible samples from the population (here we took 100 but we could have taken many more). The CI calculation essentially creates a CI with the width so that, out of 100 random samples, approximately 95 will actually contain the true population mean. As such, we can see how it is clearly a measure of the precision of the estimate. For a compelling visualization of this, look how wide the CIs for N=5 are. One thing to remember though is the ‘law of large numbers’ here. You might notice that not all of the CI plots contain exactly 5 intervals that do not contain the mean. This is analagous to the idea of coin flipping - it’s just probability over very large numbers of trials, not saying that ‘out of every 100, exactly 5 will not include the population mean’. But, over a very large number of trials, we can expect the amount of intervals that do not include the mean to tend towards 5%, in the same way that over a large number of coin flips, the proportion of heads tends towards 50%. "],["H-testing.html", "Chapter 10 Classical Statistical Hypothesis Testing 10.1 Correlation Significance Tests 10.2 Regression Significance Tests", " Chapter 10 Classical Statistical Hypothesis Testing This Chapter relates to Lecture 10, and contains some (re)worked examples from earlier sessions, focusing specifically on the hypothesis testing aspect of them. 10.1 Correlation Significance Tests Here, we revisit our correlation between GDP per capita, and Happiness metrics, which I pulled from Our Word in Data: ## Country Happiness GDPpc Pop ## Length:249 Min. :2.400 Min. : 731 Min. :8.090e+02 ## Class :character 1st Qu.:4.670 1st Qu.: 4917 1st Qu.:4.153e+05 ## Mode :character Median :5.530 Median : 12655 Median :5.596e+06 ## Mean :5.492 Mean : 20464 Mean :5.918e+07 ## 3rd Qu.:6.260 3rd Qu.: 30100 3rd Qu.:2.421e+07 ## Max. :7.820 Max. :112557 Max. :4.663e+09 ## NA&#39;s :96 NA&#39;s :52 NA&#39;s :7 ## # A tibble: 6 × 4 ## Country Happiness GDPpc Pop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 2.4 1971 38972236 ## 2 Albania 5.2 13192 2866850 ## 3 Algeria 5.12 10735 43451668 ## 4 American Samoa NA NA 46216 ## 5 Andorra NA NA 77723 ## 6 Angola NA 6110 33428490 Let’s not worry about plotting the data, and go straight to the correlation: ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 13.502, df = 146, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6636288 0.8092331 ## sample estimates: ## cor ## 0.745184 Here are our results. The estimate is a correlation, and we test that using the t statistic. The t-value is simply the estimate divided by the standard error (which we can’t see in this output), and is interpreted essentially as ‘how far from 0 is the estimate, in standard errors’. The p-value for t is very very small, and obviously less than 0.05. Conclusion - reject null hypothesis, accept alternative hypothesis (as always, pending better evidence). Importantly, this does not mean that the true correlation in the population is 0.745, simply that it is very unlikely to be zero. We can then look at our estimate of 0.745, and - even better - our confidence interval (see Section 9.2 for information on how to interpret confidence intervals), to gain some indication of the likely true correlation in the population. 10.2 Regression Significance Tests The process to asses the significance of regression estimates is very very similar to that for correlations. Let’s revisit the heart disease data set we used earlier. ## ...1 biking smoking heart.disease ## Min. : 1.0 Min. : 1.119 Min. : 0.5259 Min. : 0.5519 ## 1st Qu.:125.2 1st Qu.:20.205 1st Qu.: 8.2798 1st Qu.: 6.5137 ## Median :249.5 Median :35.824 Median :15.8146 Median :10.3853 ## Mean :249.5 Mean :37.788 Mean :15.4350 Mean :10.1745 ## 3rd Qu.:373.8 3rd Qu.:57.853 3rd Qu.:22.5689 3rd Qu.:13.7240 ## Max. :498.0 Max. :74.907 Max. :29.9467 Max. :20.4535 ## # A tibble: 6 × 4 ## ...1 biking smoking heart.disease ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.8 10.9 11.8 ## 2 2 65.1 2.22 2.85 ## 3 3 1.96 17.6 17.2 ## 4 4 44.8 2.80 6.82 ## 5 5 69.4 16.0 4.06 ## 6 6 54.4 29.3 9.55 Let’s go straight the the multiple regression model. ## ## Call: ## lm(formula = heart.disease ~ biking + smoking, data = Heart) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.1789 -0.4463 0.0362 0.4422 1.9331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.984658 0.080137 186.99 &lt;2e-16 *** ## biking -0.200133 0.001366 -146.53 &lt;2e-16 *** ## smoking 0.178334 0.003539 50.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.654 on 495 degrees of freedom ## Multiple R-squared: 0.9796, Adjusted R-squared: 0.9795 ## F-statistic: 1.19e+04 on 2 and 495 DF, p-value: &lt; 2.2e-16 We interpret these just as we did the correlation significance tests. The t-value is large, and the p-value (two-tailed) is small. Interestingly, here we are given ‘stars’ for the different levels of significance, so to some extent the software is doing some decision making for you. To be honest, I always caution against relying solely on looking for ‘stars’ (it’s actually a bit of a running joke that I once told an entire class in the 1990s to ‘just look at the stars’). That’s because the actual significance or not decision is based on the critical value and one- or two-tailed decision. The software often makes an assumption of 0.05 critical value for p, two-tailed, and calculates the ‘stars’ based on that. Sometimes that can conflict with the decision you have made yourself about what should be significant or not. That can trip you up if you didn’t know to change these values in the software package. Further, it also sort of entrenches the idea that things can be ‘more’ or ‘less’ statistically significant. Take a look at the results above, you’ll see 3 stars represents a significance of ‘0’, and 2 stars represents ‘0.001’, boring old ‘0.05’ only gets a single star, and ‘0.1’ gets a dot. I’m not a fan here because this encourages the analyst to post-hoc make decisions about ‘marginally’ significant, or ‘very’ significant. These concepts do not exist. You decide your critical value, and you either pass or fail it. Am I perfect? No. Specifically, do any of my papers use the language of ‘marginal’ significance? Sure, I bet you could find it. I am never a fan though, and I can promise you I argued about it at the time! "],["ANOVA.html", "Chapter 11 ANOVA 11.1 Post-Hoc Testing", " Chapter 11 ANOVA This Chapter provides analysis and examples related to the first part of Lecture 11. Here, I’ll demonstrate the basic application of ANOVA on the simple 3-group case of the Ed Sheeran Study, and go from there to introduce some classic ‘concerns’ with the statistical hypothesis testing methodology. ## ID GROUP ANGER ## Min. : 1 1:15 Min. :1.000 ## 1st Qu.:12 2:15 1st Qu.:2.000 ## Median :23 3:15 Median :3.000 ## Mean :23 Mean :3.222 ## 3rd Qu.:34 3rd Qu.:4.000 ## Max. :45 Max. :5.000 ## # A tibble: 6 × 3 ## ID GROUP ANGER ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 2 5 ## 3 3 1 2 ## 4 4 3 3 ## 5 5 2 4 ## 6 6 3 2 Let’s create a quick table of the group means, and visualize it with a simple boxplot of the groups. ## # A tibble: 3 × 5 ## GROUP variable n mean sd ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 ANGER 15 2.33 0.816 ## 2 2 ANGER 15 4.27 0.704 ## 3 3 ANGER 15 3.07 1.03 Interesting picture. Group 1 is the control group, Group 2 is those who heard Ed Sheeran, Group 3 is those who heard other music. This is what we might call ‘model-free’ analysis. We are just looking at the data to see if intuitively it looks like we expect it to. Let’s run an ANOVA now, which places a statistical model on it, and tests whether or not the data supports the model. ## ANOVA Table (type II tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 GROUP 2 42 19.235 1.17e-06 * 0.478 Results here suggest there is a significant effect (p-value is very small). We also have an effect size measure (‘ges’, or generalized eta-squared) which is very useful to us, and suggests the effect is quite large. This can be interpreted similarly to a regression coefficient (which is also an effect size measure), and is the amount of variance in the dependent variable (Anger) that is explained by group membership. However, ANOVA only tests the general effect of the treatment / group. We don’t know whether this is because of the difference between all of the groups, or only some. E.g., is it that there is an effect of music in general (i.e. between Control and Ed, and Control and Music, but not between Ed and Music), or that Ed specifically is anger-inducing (in which case we would see an effect between Ed and Music, and Ed and Control, and not between Music and Control). 11.1 Post-Hoc Testing We can investigate this using post-hoc tests, which compare the difference between each pair of groups. There are many different types of post-hoc test for ANOVA, but the most typical one to use is the Tukey’s test, which is what we will do here. ## # A tibble: 3 × 9 ## term group1 group2 null.value estimate conf.low conf.high p.adj ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GROUP 1 2 0 1.93 1.17 2.70 0.000000734 ## 2 GROUP 1 3 0 0.733 -0.0313 1.50 0.0625 ## 3 GROUP 2 3 0 -1.2 -1.96 -0.435 0.00126 ## # ℹ 1 more variable: p.adj.signif &lt;chr&gt; We can actually plot these results in a really effective way: This very clearly tells us that it is the Ed Sheeran group (2) that is driving these results, and there isn’t much to choose between the control group, and the ‘music’ group. However, this type of post-hoc analysis has the potential for what is known as a multiple comparisons problem, which we might need to deal with. Let’s take a look back in the slide deck… Remember, there are many other things that if we were doing ANOVA that we would also look to deal with - such as the various assumptions required of ANOVA, and so forth. But, they are beyond our scope in this class. Suffice to say that this has only scratched the surface of ANOVA so far. "],["Issues.html", "Chapter 12 Issues with Significance Testing 12.1 More on the Multiple Comparisons Problem 12.2 The Bonferroni Correction: Rate of Change in Football Goals per Season 12.3 Statistical Power: Brief Demonstration", " Chapter 12 Issues with Significance Testing This chapter provides examples and analysis for the second part of Lecture 11. In this chapter, I’ll explore examples of two common ‘issues’ or ‘debates’ regarding statistical significance testing, applied to two of our prior examples. But first, let’s unpack more about the Ed Sheeran Study, and the multiple comparisons problem. 12.1 More on the Multiple Comparisons Problem In this section, I’ll provide some visualizations which can help us to understand the problem with multiple comparisons. However, it’s worth noting that the results depend on a number of assumptions about the research and what the real world actually looks like. The latter for sure we can never know. So, do bear that in mind. First, let’s visualize the situation I just discussed in the slides. We test 100 hypotheses, and in every case, the null is true in the population (remember, we cannot know the true state of the population in reality). Below, each grey box represents a hypothesis we test. Now, remember, even if all null hypotheses are true, our significance level, or p = 0.05, so we have to expect that on average we would expect 5 of these hypotheses tests to return p-values &lt; 0.05, and thus we would reject the null. This is known as a false positive. You might wonder how we could reduce the chances of false positive to zero. You could do this by setting your required p to be exactly zero. Do you see the problem here? You would certainly solve the false positive problem. But, you would also accept all null hypotheses. You would never actually find a statistically significant result in your research. Unless the null hypothesis really was true in the population in every single case, you would be inevitably missing out on making some discoveries. Moving on, let’s visualize the situation in hand here. Below, the same 100 hypothesis tests (remember, the null is true in all cases), but this time I have marked in red the expected level of false positives. Now, that is just the expected level for every 100 tests. If you remember from the discussion in Section 9.2, it may not be you get exactly 5 false positives in 100 tests, just that you would expect the long run average number of false positives to be 5%, in an all-null world. So, we can look at it through a different lens. What is the probability that you would get at least one false positive in any given batch of multiple tests. That is known as the familywise error rate, and it applies whenever you can define a cluster or ‘family’ of tests. It’s the same principle as rolling a 6-sided die, and needing to score a ‘6’. Think of the ‘6’ as the ‘false positive’ here. In a single die role, the probability of getting a ‘6’ is \\(\\frac{1}{6}\\), and of course the equivalent probability of not getting a ‘6’ is \\(1-\\frac{1}{6}=\\frac{5}{6}\\). But, what if you get to roll the die 3 times? It’s fairly easy to intuit that, even though the probability of getting a ‘6’ on any single roll stays the same, the probability of getting a single ‘6’ in 3 rolls is higher. In fact, we can calculate it. To do so, we use the probability of not getting a 6, as follows: P(no 6)=\\((\\frac{5}{6})^{3} = 0.579\\) Recalling the laws of probability then… P(at least one 6) = 1 - P(no 6) = 1 - .579 = .421 So, now we have a 42% probability of at least one 6 in 3 rolls, even though the probability of getting a 6 in any single roll is unchanged. We can calculate the familywise error rate for any set of multiple comparisons in exactly the same way. One of the key advantages of ANOVA is that it is a single test, and thus does not fall prey to the multiple comparisons problem. However, in the case of post-hoc testing for the Ed Sheeran study, we are doing 3 pairwise comparisons (i.e. 3 tests) across the 3 groups, with a stated significance level of 0.05. For 1 test, the false positive rate is \\(1 - 0.95 = .05\\) But, for 3 tests, the false positive rate is \\(1-0.95^{3}=0.14\\) We are ‘rolling the dice’ multiple times. In fact, below is a nice little calculator that demonstrates how the familywise error rate changes, based on the number of tests, and the chosen significance level. It was created by Dr Daniel Roelfs, during his time at the Norwegian Centre for Mental Disorders Research (NORMENT) in Oslo, and he kindly allowed me to use it here. Check out his website: https://danielroelfs.com/about/ What you could do is adjust the sliders to check the above calculations - what is the familywise error rate for 3 comparisons and a 0.05 significance level? When you adjust the slides, it feels kind of scary right? There are many ways to deal with the problem, but they pretty much all amount to making some correction to the required significance level, making it more stringent, in order to reduce the false positives. For example, in the Ed Sheeran post-hoc tests above, we used Tukey’s test, which uses a correction for multiple comparisons. Another method dealt with below is the Bonferoni Correction, which is generally the most well-known of them. 12.2 The Bonferroni Correction: Rate of Change in Football Goals per Season Here, I’m using data from https://www.footballhistory.org/league/premier-league-statistics.html I hand-entered this into a spreadsheet, and calculated the additional numbers myself. ## # A tibble: 6 × 7 ## Season Games Goals GPG SE Lower95CI Upper95CI ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-96 380 988 2.6 31.4 926. 1050. ## 2 1996-97 380 970 2.55 31.1 909. 1031. ## 3 1997-98 380 1019 2.68 31.9 956. 1082. ## 4 1998-99 380 959 2.52 31.0 898. 1020. ## 5 1999-00 380 1060 2.79 32.6 996. 1124. ## 6 2000-01 380 992 2.61 31.5 930. 1054. You can see here I have calculated the standard errors from the yearly goal totals (which represent that year’s underlying rate of goal occurrence), then used that to calculate the 95% confidence Interval limits We can use these to create a nifty chart with error bars, drawing from the code used by Spiegelhalter for Figure 9.4 in his book, available on his github: https://github.com/dspiegel29/ArtofStatistics/blob/master/09-4-homicide-rates-E%2BW/09-4-homicide-trends-x.Rmd From this chart, and looking at the data itself, we can see that the 95% Intervals overlap, so it is hard to conclude that the underlying rate of goals has changed significantly year on year. Yes, even in the pandemic, despite what many football ‘experts’ said. We can look at this in some more depth though. First, it’s worth knowing that the UK Office for National Statistics uses this basic technique to estimate the probability of homicides, which also seem to be usefully approximated by the Poisson distribution. Interestingly, the ONS suggest that it is over-stringent to rely on error bar overlap, and that we can also use z-tests to directly test the assumption that the change is zero. See: https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/compendium/focusonviolentcrimeandsexualoffences/yearendingmarch2016/homicide#statistical-interpretation-of-trends-in-homicides So using the z-scores in the data file, I have calculated the p-value (2 tailed as we do not hypothesize a direction for the difference) for the z-scores for the difference between each season, year-on-year. ## Season Games Goals GPG ## Length:27 Min. :380 Min. : 931.0 Min. :2.450 ## Class :character 1st Qu.:380 1st Qu.: 981.5 1st Qu.:2.583 ## Mode :character Median :380 Median :1018.0 Median :2.679 ## Mean :380 Mean :1013.9 Mean :2.668 ## 3rd Qu.:380 3rd Qu.:1056.5 3rd Qu.:2.780 ## Max. :380 Max. :1072.0 Max. :2.821 ## ## SE Lower95CI Upper95CI Change ## Min. :30.51 Min. : 871.2 Min. : 990.8 Min. :-77.000 ## 1st Qu.:31.33 1st Qu.: 920.1 1st Qu.:1042.9 1st Qu.:-35.500 ## Median :31.91 Median : 955.5 Median :1080.5 Median : -2.000 ## Mean :31.84 Mean : 951.5 Mean :1076.3 Mean : 3.192 ## 3rd Qu.:32.50 3rd Qu.: 992.8 3rd Qu.:1120.2 3rd Qu.: 44.750 ## Max. :32.74 Max. :1007.8 Max. :1136.2 Max. :111.000 ## NA&#39;s :1 ## Z negged p2 ## Min. :-1.71027 Min. :-2.48514 Min. :0.01295 ## 1st Qu.:-0.79795 1st Qu.:-1.30711 1st Qu.:0.19236 ## Median :-0.04369 Median :-0.83063 Median :0.40618 ## Mean : 0.07032 Mean :-0.88179 Mean :0.46986 ## 3rd Qu.: 0.97794 3rd Qu.:-0.24627 3rd Qu.:0.80549 ## Max. : 2.48514 Max. :-0.02236 Max. :0.98216 ## NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 ## # A tibble: 6 × 11 ## Season Games Goals GPG SE Lower95CI Upper95CI Change Z negged ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1995-96 380 988 2.6 31.4 926. 1050. NA NA NA ## 2 1996-97 380 970 2.55 31.1 909. 1031. -18 -0.407 -0.407 ## 3 1997-98 380 1019 2.68 31.9 956. 1082. 49 1.10 -1.10 ## 4 1998-99 380 959 2.52 31.0 898. 1020. -60 -1.35 -1.35 ## 5 1999-00 380 1060 2.79 32.6 996. 1124. 101 2.25 -2.25 ## 6 2000-01 380 992 2.61 31.5 930. 1054. -68 -1.50 -1.50 ## # ℹ 1 more variable: p2 &lt;dbl&gt; Here, we can plot the p-values (2-tailed), and again we see that two seasons seem to have significant differences. In other words, the p-values are less than 0.05 for the test as to whether the number of goals scored differs from the season before We can see that the 1999-2000 season, and the 2009-10 seasons have p values less than 0.05 The question is are we suffering from the multiple comparisons problem? Should we correct for it? It’s hard to say actually. Of course, we are indeed running multiple tests, 26 in fact. So, the chance of a false positive is high. The Bonferroni correction would immediately reduce the false positive chances, but at what cost? Let’s see how this would work. In order to calculate a Bonferroni correction, you can either adjust the p-value directly that you calculate for each test, or instead simply adjust the ‘cutoff’ value for p, known as the critical p, to lower it from 0.05 and make the significance test ‘harder’ to pass. The formula to adjust the cutoff value is simply: \\(\\alpha_b = (\\frac{\\alpha}{n})\\) Where \\(\\alpha_b\\) = Bonferroni-adjusted critical p value \\(\\alpha\\) = original critical p value (here this is 0.05) n = number of comparisons (here this is 26) So, the formula gives us a new Bonferroni-adjusted critical p value of 0.0019 Let’s see what happens with this new critical p value: We can see that none of our tests now rejects the null. The Bonferroni correction is known as a highly conservative test. That is, it is based on the idea that the null hypothesis is true in each case in the population. We can thus consider it as the most stringent and conservative way to correct for the chance of false positives when doing multiple comparisons. But, as I suggested above, that might not always be the best idea. Indeed, if you want to totally avoid any chance of a false positive, why not simply make the required alpha 0? Then, you would never get a false positive. Of course, you would never detect a true positive either. What if it is the alternative hypothesis (that is, the H of an effect existing) that is true in all cases? In such cases, there can of course be no false positives. Therefore, in such a situation you would be increasing the chances of a false negative by reducing the chances of a false positive. So, what are the potential costs of each of these mistakes? For example, Thomas Perneger’s 1998 paper in the BMJ is scathing about the Bonferroni adjustment. Take a look at https://www.bmj.com/content/316/7139/1236.full Mind you, I am not saying that’s the final word, just that there are multiple perspectives on the issues! It’s never as simple as it seems when making statistical decisions, is it? 12.3 Statistical Power: Brief Demonstration Different types of analysis and research design require different types of power calculation, so it is hard to give a uniform example. But, for simplicity’s sake, let’s calculate the required sample size for the Ed Sheeran study we conducted earlier. Remember, really, we should have done this before collecting data. To calculate power, all we need the parameters of the experiment and analysis design. As such, it is easily possible to do this before collecting data, and to design your studies around it. Really, we should do this a lot more in business and management - it’s routine in fields like medicine. So, we had 3 groups, and used ANOVA Let’s set a significance of 0.05, a required power of 0.8, and assume the effect size is moderate (say 0.25) ## ## Balanced one-way analysis of variance power calculation ## ## k = 3 ## n = 52.3966 ## f = 0.25 ## sig.level = 0.05 ## power = 0.8 ## ## NOTE: n is number in each group So, we really wanted to have around 50 in each group to have an 80% chance of detecting a moderate effect presuming the null was true. You can see that my study (with only 15 in each group) was rather underpowered. However, if I had increased the effect size in the calculation to 0.5 (close to what the experiment suggested) this would have given me a result for n closer to what I actually used. However, you’d have to be VERY confident in the size of your likely effect to actually do that I think. "],["references.html", "References", " References When creating this book, I have used data from numerous publicly-available sources. I have also sometimes used code from publicly-available repositories and websites. All sources are cited at the place they are used at this point, but in the future will be compiled here Further, a number of open-access academic papers and other reports are cited or referred to in Chapters. Weblinks are provided for all sources mentioned above where possible. However, any direct queries can be emailed to me, in case I have missed something, or perhaps a URL no longer works (all checked as of January 2024.) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
